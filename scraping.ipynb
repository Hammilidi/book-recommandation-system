{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0e691f11-f1c1-44b0-8456-4d6fafab5fc8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: psycopg2 in c:\\users\\dell\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (2.9.10)\n",
      "Requirement already satisfied: requests in c:\\users\\dell\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (2.32.3)\n",
      "Collecting beautifulsoup4\n",
      "  Downloading beautifulsoup4-4.13.4-py3-none-any.whl.metadata (3.8 kB)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\dell\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from requests) (3.4.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\dell\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from requests) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\dell\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from requests) (2.4.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\dell\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from requests) (2025.4.26)\n",
      "Collecting soupsieve>1.2 (from beautifulsoup4)\n",
      "  Downloading soupsieve-2.7-py3-none-any.whl.metadata (4.6 kB)\n",
      "Requirement already satisfied: typing-extensions>=4.0.0 in c:\\users\\dell\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from beautifulsoup4) (4.14.0)\n",
      "Downloading beautifulsoup4-4.13.4-py3-none-any.whl (187 kB)\n",
      "Downloading soupsieve-2.7-py3-none-any.whl (36 kB)\n",
      "Installing collected packages: soupsieve, beautifulsoup4\n",
      "Successfully installed beautifulsoup4-4.13.4 soupsieve-2.7\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 25.0.1 -> 25.2\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "%pip install psycopg2 requests beautifulsoup4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "7d66db4c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Données sauvegardées dans livres.csv\n",
      "✅ Données sauvegardées dans PostgreSQL avec to_sql()\n"
     ]
    }
   ],
   "source": [
    "import csv\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "from sqlalchemy import create_engine\n",
    "\n",
    "# -----------------------\n",
    "# Config PostgreSQL\n",
    "# -----------------------\n",
    "DB_URI = \"postgresql+psycopg2://postgres:admin@localhost:5432/booksdb\"\n",
    "\n",
    "base_url = \"https://books.toscrape.com/catalogue/page-{}.html\"\n",
    "livres = []\n",
    "\n",
    "rating_map = {\n",
    "    \"One\": \"1\",\n",
    "    \"Two\": \"2\",\n",
    "    \"Three\": \"3\",\n",
    "    \"Four\": \"4\",\n",
    "    \"Five\": \"5\"\n",
    "}\n",
    "\n",
    "# ======= SCRAPING =======\n",
    "for page in range(1, 51):\n",
    "    url = base_url.format(page)\n",
    "    response = requests.get(url)\n",
    "    response.encoding = \"latin-1\"\n",
    "    \n",
    "    soup = BeautifulSoup(response.text, \"html.parser\")\n",
    "    articles = soup.find_all(\"article\", class_=\"product_pod\")\n",
    "    \n",
    "    for article in articles:\n",
    "        titre = article.h3.a[\"title\"]\n",
    "        titre = titre.encode(\"latin-1\", errors=\"ignore\").decode(\"utf-8\", errors=\"ignore\")\n",
    "        \n",
    "        prix_str = article.find(\"p\", class_=\"price_color\").text.strip()\n",
    "        prix_str = prix_str.replace(\"Â\", \"\").replace(\"£\", \"\")\n",
    "        prix = float(prix_str)\n",
    "        \n",
    "        stock = article.find(\"p\", class_=\"instock availability\").text.strip()\n",
    "        stock = stock.encode(\"latin-1\", errors=\"ignore\").decode(\"utf-8\", errors=\"ignore\")\n",
    "        \n",
    "        img_url = article.find(\"img\")[\"src\"].replace(\"../../\", \"https://books.toscrape.com/\")\n",
    "        \n",
    "        rating_class = article.find(\"p\", class_=\"star-rating\")[\"class\"][1]\n",
    "        rating = int(rating_map.get(rating_class, \"0\"))\n",
    "        \n",
    "        livres.append([titre, prix, stock, img_url, rating])\n",
    "\n",
    "# Sauvegarde CSV en UTF-8\n",
    "csv_file = \"livres.csv\"\n",
    "with open(csv_file, \"w\", newline=\"\", encoding=\"utf-8\") as f:\n",
    "    writer = csv.writer(f)\n",
    "    writer.writerow([\"Titre\", \"Prix\", \"Stock\", \"Image URL\", \"Rating\"])\n",
    "    writer.writerows(livres)\n",
    "\n",
    "print(\"✅ Données sauvegardées dans livres.csv\")\n",
    "\n",
    "# ======= CHARGEMENT CSV & STOCKAGE DB =======\n",
    "try:\n",
    "    # Charger le CSV\n",
    "    df = pd.read_csv(csv_file, encoding=\"utf-8\")\n",
    "    \n",
    "    # Connexion PostgreSQL via SQLAlchemy\n",
    "    engine = create_engine(DB_URI)\n",
    "    \n",
    "    # Sauvegarde dans la table \"livres\"\n",
    "    df.to_sql(\"livres\", engine, if_exists=\"replace\", index=False)\n",
    "    \n",
    "    print(\"✅ Données sauvegardées dans PostgreSQL avec to_sql()\")\n",
    "except Exception as e:\n",
    "    print(\"❌ Erreur lors de la sauvegarde dans PostgreSQL :\", e)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "80548c4f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Données sauvegardées dans livres.csv\n",
      "✅ Données sauvegardées dans PostgreSQL avec to_sql()\n"
     ]
    }
   ],
   "source": [
    "import csv\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "from sqlalchemy import create_engine\n",
    "from urllib.parse import urljoin\n",
    "\n",
    "# -----------------------\n",
    "# Config PostgreSQL\n",
    "# -----------------------\n",
    "DB_URI = \"postgresql+psycopg2://postgres:admin@localhost:5432/booksdb\"\n",
    "\n",
    "base_url = \"https://books.toscrape.com/catalogue/page-{}.html\"\n",
    "site_base = \"https://books.toscrape.com/\"\n",
    "livres = []\n",
    "\n",
    "rating_map = {\n",
    "    \"One\": \"1\",\n",
    "    \"Two\": \"2\",\n",
    "    \"Three\": \"3\",\n",
    "    \"Four\": \"4\",\n",
    "    \"Five\": \"5\"\n",
    "}\n",
    "\n",
    "# ======= SCRAPING =======\n",
    "for page in range(1, 51):\n",
    "    url = base_url.format(page)\n",
    "    response = requests.get(url)\n",
    "    response.encoding = \"latin-1\"\n",
    "    \n",
    "    soup = BeautifulSoup(response.text, \"html.parser\")\n",
    "    articles = soup.find_all(\"article\", class_=\"product_pod\")\n",
    "    \n",
    "    for article in articles:\n",
    "        titre = article.h3.a[\"title\"]\n",
    "        titre = titre.encode(\"latin-1\", errors=\"ignore\").decode(\"utf-8\", errors=\"ignore\")\n",
    "        \n",
    "        prix_str = article.find(\"p\", class_=\"price_color\").text.strip()\n",
    "        prix_str = prix_str.replace(\"Â\", \"\").replace(\"£\", \"\")\n",
    "        prix = float(prix_str)\n",
    "        \n",
    "        stock = article.find(\"p\", class_=\"instock availability\").text.strip()\n",
    "        stock = stock.encode(\"latin-1\", errors=\"ignore\").decode(\"utf-8\", errors=\"ignore\")\n",
    "        \n",
    "        img_url = article.find(\"img\")[\"src\"]\n",
    "        img_url = urljoin(site_base, img_url)  # construit une URL absolue\n",
    "        \n",
    "        rating_class = article.find(\"p\", class_=\"star-rating\")[\"class\"][1]\n",
    "        rating = int(rating_map.get(rating_class, \"0\"))\n",
    "        \n",
    "        # --- Récupération de la description ---\n",
    "        detail_href = article.h3.a[\"href\"]\n",
    "        detail_url = urljoin(site_base + \"catalogue/\", detail_href)  # URL absolue\n",
    "        detail_resp = requests.get(detail_url)\n",
    "        detail_resp.encoding = \"latin-1\"\n",
    "        detail_soup = BeautifulSoup(detail_resp.text, \"html.parser\")\n",
    "        \n",
    "        desc_tag = detail_soup.find(\"div\", id=\"product_description\")\n",
    "        if desc_tag:\n",
    "            description = desc_tag.find_next_sibling(\"p\").text.strip()\n",
    "        else:\n",
    "            description = \"\"\n",
    "        \n",
    "        livres.append([titre, prix, stock, img_url, rating, description])\n",
    "\n",
    "# Sauvegarde CSV en UTF-8\n",
    "csv_file = \"livres.csv\"\n",
    "with open(csv_file, \"w\", newline=\"\", encoding=\"utf-8\") as f:\n",
    "    writer = csv.writer(f)\n",
    "    writer.writerow([\"Titre\", \"Prix\", \"Stock\", \"Image URL\", \"Rating\", \"Description\"])\n",
    "    writer.writerows(livres)\n",
    "\n",
    "print(\"✅ Données sauvegardées dans livres.csv\")\n",
    "\n",
    "# ======= CHARGEMENT CSV & STOCKAGE DB =======\n",
    "try:\n",
    "    # Charger le CSV\n",
    "    df = pd.read_csv(csv_file, encoding=\"utf-8\")\n",
    "    \n",
    "    # Connexion PostgreSQL via SQLAlchemy\n",
    "    engine = create_engine(DB_URI)\n",
    "    \n",
    "    # Sauvegarde dans la table \"livres\"\n",
    "    df.to_sql(\"livres\", engine, if_exists=\"replace\", index=False)\n",
    "    \n",
    "    print(\"✅ Données sauvegardées dans PostgreSQL avec to_sql()\")\n",
    "except Exception as e:\n",
    "    print(\"❌ Erreur lors de la sauvegarde dans PostgreSQL :\", e)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "180d76fb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "5945c406",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Dell\\AppData\\Local\\Temp\\ipykernel_13856\\3077488864.py:37: MovedIn20Warning: The ``declarative_base()`` function is now available as sqlalchemy.orm.declarative_base(). (deprecated since: 2.0) (Background on SQLAlchemy 2.0 at: https://sqlalche.me/e/b8d9)\n",
      "  Base = declarative_base()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== SCRAPING BOOKS TO SCRAPE AVEC SELENIUM ===\n",
      "\n",
      "Début du scraping...\n",
      "Scraping page 1...\n",
      "Trouvé 20 livres sur la page 1\n",
      "  Scraping livre 1/20...\n",
      "  Scraping livre 2/20...\n",
      "  Scraping livre 3/20...\n",
      "  Scraping livre 4/20...\n",
      "  Scraping livre 5/20...\n",
      "  Scraping livre 6/20...\n",
      "  Scraping livre 7/20...\n",
      "  Scraping livre 8/20...\n",
      "  Scraping livre 9/20...\n",
      "  Scraping livre 10/20...\n",
      "  Scraping livre 11/20...\n",
      "  Scraping livre 12/20...\n",
      "  Scraping livre 13/20...\n",
      "  Scraping livre 14/20...\n",
      "  Scraping livre 15/20...\n",
      "  Scraping livre 16/20...\n",
      "  Scraping livre 17/20...\n",
      "  Scraping livre 18/20...\n",
      "  Scraping livre 19/20...\n",
      "  Scraping livre 20/20...\n",
      "Scraping page 2...\n",
      "Trouvé 20 livres sur la page 2\n",
      "  Scraping livre 1/20...\n",
      "  Scraping livre 2/20...\n",
      "  Scraping livre 3/20...\n",
      "  Scraping livre 4/20...\n",
      "  Scraping livre 5/20...\n",
      "  Scraping livre 6/20...\n",
      "  Scraping livre 7/20...\n",
      "  Scraping livre 8/20...\n",
      "  Scraping livre 9/20...\n",
      "  Scraping livre 10/20...\n",
      "  Scraping livre 11/20...\n",
      "  Scraping livre 12/20...\n",
      "  Scraping livre 13/20...\n",
      "  Scraping livre 14/20...\n",
      "  Scraping livre 15/20...\n",
      "  Scraping livre 16/20...\n",
      "  Scraping livre 17/20...\n",
      "  Scraping livre 18/20...\n",
      "  Scraping livre 19/20...\n",
      "  Scraping livre 20/20...\n",
      "Scraping page 3...\n",
      "Trouvé 20 livres sur la page 3\n",
      "  Scraping livre 1/20...\n",
      "  Scraping livre 2/20...\n",
      "  Scraping livre 3/20...\n",
      "  Scraping livre 4/20...\n",
      "  Scraping livre 5/20...\n",
      "  Scraping livre 6/20...\n",
      "  Scraping livre 7/20...\n",
      "  Scraping livre 8/20...\n",
      "  Scraping livre 9/20...\n",
      "  Scraping livre 10/20...\n",
      "  Scraping livre 11/20...\n",
      "  Scraping livre 12/20...\n",
      "  Scraping livre 13/20...\n",
      "  Scraping livre 14/20...\n",
      "  Scraping livre 15/20...\n",
      "  Scraping livre 16/20...\n",
      "  Scraping livre 17/20...\n",
      "  Scraping livre 18/20...\n",
      "  Scraping livre 19/20...\n",
      "  Scraping livre 20/20...\n",
      "Scraping page 4...\n",
      "Trouvé 20 livres sur la page 4\n",
      "  Scraping livre 1/20...\n",
      "  Scraping livre 2/20...\n",
      "  Scraping livre 3/20...\n",
      "  Scraping livre 4/20...\n",
      "  Scraping livre 5/20...\n",
      "  Scraping livre 6/20...\n",
      "  Scraping livre 7/20...\n",
      "  Scraping livre 8/20...\n",
      "  Scraping livre 9/20...\n",
      "  Scraping livre 10/20...\n",
      "  Scraping livre 11/20...\n",
      "  Scraping livre 12/20...\n",
      "  Scraping livre 13/20...\n",
      "  Scraping livre 14/20...\n",
      "  Scraping livre 15/20...\n",
      "  Scraping livre 16/20...\n",
      "  Scraping livre 17/20...\n",
      "  Scraping livre 18/20...\n",
      "  Scraping livre 19/20...\n",
      "  Scraping livre 20/20...\n",
      "Scraping page 5...\n",
      "Trouvé 20 livres sur la page 5\n",
      "  Scraping livre 1/20...\n",
      "  Scraping livre 2/20...\n",
      "  Scraping livre 3/20...\n",
      "  Scraping livre 4/20...\n",
      "  Scraping livre 5/20...\n",
      "  Scraping livre 6/20...\n",
      "  Scraping livre 7/20...\n",
      "  Scraping livre 8/20...\n",
      "  Scraping livre 9/20...\n",
      "  Scraping livre 10/20...\n",
      "  Scraping livre 11/20...\n",
      "  Scraping livre 12/20...\n",
      "  Scraping livre 13/20...\n",
      "  Scraping livre 14/20...\n",
      "  Scraping livre 15/20...\n",
      "  Scraping livre 16/20...\n",
      "  Scraping livre 17/20...\n",
      "  Scraping livre 18/20...\n",
      "  Scraping livre 19/20...\n",
      "  Scraping livre 20/20...\n",
      "Scraping page 6...\n",
      "Trouvé 20 livres sur la page 6\n",
      "  Scraping livre 1/20...\n",
      "  Scraping livre 2/20...\n",
      "  Scraping livre 3/20...\n",
      "  Scraping livre 4/20...\n",
      "  Scraping livre 5/20...\n",
      "  Scraping livre 6/20...\n",
      "  Scraping livre 7/20...\n",
      "  Scraping livre 8/20...\n",
      "  Scraping livre 9/20...\n",
      "  Scraping livre 10/20...\n",
      "  Scraping livre 11/20...\n",
      "  Scraping livre 12/20...\n",
      "  Scraping livre 13/20...\n",
      "  Scraping livre 14/20...\n",
      "  Scraping livre 15/20...\n",
      "  Scraping livre 16/20...\n",
      "  Scraping livre 17/20...\n",
      "  Scraping livre 18/20...\n",
      "  Scraping livre 19/20...\n",
      "  Scraping livre 20/20...\n",
      "Scraping page 7...\n",
      "Trouvé 20 livres sur la page 7\n",
      "  Scraping livre 1/20...\n",
      "  Scraping livre 2/20...\n",
      "  Scraping livre 3/20...\n",
      "  Scraping livre 4/20...\n",
      "  Scraping livre 5/20...\n",
      "  Scraping livre 6/20...\n",
      "  Scraping livre 7/20...\n",
      "  Scraping livre 8/20...\n",
      "  Scraping livre 9/20...\n",
      "  Scraping livre 10/20...\n",
      "  Scraping livre 11/20...\n",
      "  Scraping livre 12/20...\n",
      "  Scraping livre 13/20...\n",
      "  Scraping livre 14/20...\n",
      "  Scraping livre 15/20...\n",
      "  Scraping livre 16/20...\n",
      "  Scraping livre 17/20...\n",
      "  Scraping livre 18/20...\n",
      "  Scraping livre 19/20...\n",
      "  Scraping livre 20/20...\n",
      "Scraping page 8...\n",
      "Trouvé 20 livres sur la page 8\n",
      "  Scraping livre 1/20...\n",
      "  Scraping livre 2/20...\n",
      "  Scraping livre 3/20...\n",
      "  Scraping livre 4/20...\n",
      "  Scraping livre 5/20...\n",
      "  Scraping livre 6/20...\n",
      "  Scraping livre 7/20...\n",
      "  Scraping livre 8/20...\n",
      "  Scraping livre 9/20...\n",
      "  Scraping livre 10/20...\n",
      "  Scraping livre 11/20...\n",
      "  Scraping livre 12/20...\n",
      "  Scraping livre 13/20...\n",
      "  Scraping livre 14/20...\n",
      "  Scraping livre 15/20...\n",
      "  Scraping livre 16/20...\n",
      "  Scraping livre 17/20...\n",
      "  Scraping livre 18/20...\n",
      "  Scraping livre 19/20...\n",
      "  Scraping livre 20/20...\n",
      "Scraping page 9...\n",
      "Trouvé 20 livres sur la page 9\n",
      "  Scraping livre 1/20...\n",
      "  Scraping livre 2/20...\n",
      "  Scraping livre 3/20...\n",
      "  Scraping livre 4/20...\n",
      "  Scraping livre 5/20...\n",
      "  Scraping livre 6/20...\n",
      "  Scraping livre 7/20...\n",
      "  Scraping livre 8/20...\n",
      "  Scraping livre 9/20...\n",
      "  Scraping livre 10/20...\n",
      "  Scraping livre 11/20...\n",
      "  Scraping livre 12/20...\n",
      "  Scraping livre 13/20...\n",
      "  Scraping livre 14/20...\n",
      "  Scraping livre 15/20...\n",
      "  Scraping livre 16/20...\n",
      "  Scraping livre 17/20...\n",
      "  Scraping livre 18/20...\n",
      "  Scraping livre 19/20...\n",
      "  Scraping livre 20/20...\n",
      "Scraping page 10...\n",
      "Trouvé 20 livres sur la page 10\n",
      "  Scraping livre 1/20...\n",
      "  Scraping livre 2/20...\n",
      "  Scraping livre 3/20...\n",
      "  Scraping livre 4/20...\n",
      "  Scraping livre 5/20...\n",
      "  Scraping livre 6/20...\n",
      "  Scraping livre 7/20...\n",
      "  Scraping livre 8/20...\n",
      "  Scraping livre 9/20...\n",
      "  Scraping livre 10/20...\n",
      "  Scraping livre 11/20...\n",
      "  Scraping livre 12/20...\n",
      "  Scraping livre 13/20...\n",
      "  Scraping livre 14/20...\n",
      "  Scraping livre 15/20...\n",
      "  Scraping livre 16/20...\n",
      "  Scraping livre 17/20...\n",
      "  Scraping livre 18/20...\n",
      "  Scraping livre 19/20...\n",
      "  Scraping livre 20/20...\n",
      "Scraping page 11...\n",
      "Trouvé 20 livres sur la page 11\n",
      "  Scraping livre 1/20...\n",
      "  Scraping livre 2/20...\n",
      "  Scraping livre 3/20...\n",
      "  Scraping livre 4/20...\n",
      "  Scraping livre 5/20...\n",
      "  Scraping livre 6/20...\n",
      "  Scraping livre 7/20...\n",
      "  Scraping livre 8/20...\n",
      "  Scraping livre 9/20...\n",
      "  Scraping livre 10/20...\n",
      "  Scraping livre 11/20...\n",
      "  Scraping livre 12/20...\n",
      "  Scraping livre 13/20...\n",
      "  Scraping livre 14/20...\n",
      "  Scraping livre 15/20...\n",
      "  Scraping livre 16/20...\n",
      "  Scraping livre 17/20...\n",
      "  Scraping livre 18/20...\n",
      "  Scraping livre 19/20...\n",
      "  Scraping livre 20/20...\n",
      "Scraping page 12...\n",
      "Trouvé 20 livres sur la page 12\n",
      "  Scraping livre 1/20...\n",
      "  Scraping livre 2/20...\n",
      "  Scraping livre 3/20...\n",
      "  Scraping livre 4/20...\n",
      "  Scraping livre 5/20...\n",
      "  Scraping livre 6/20...\n",
      "  Scraping livre 7/20...\n",
      "  Scraping livre 8/20...\n",
      "  Scraping livre 9/20...\n",
      "  Scraping livre 10/20...\n",
      "  Scraping livre 11/20...\n",
      "  Scraping livre 12/20...\n",
      "  Scraping livre 13/20...\n",
      "  Scraping livre 14/20...\n",
      "  Scraping livre 15/20...\n",
      "  Scraping livre 16/20...\n",
      "  Scraping livre 17/20...\n",
      "  Scraping livre 18/20...\n",
      "  Scraping livre 19/20...\n",
      "  Scraping livre 20/20...\n",
      "Scraping page 13...\n",
      "Trouvé 20 livres sur la page 13\n",
      "  Scraping livre 1/20...\n",
      "  Scraping livre 2/20...\n",
      "  Scraping livre 3/20...\n",
      "  Scraping livre 4/20...\n",
      "  Scraping livre 5/20...\n",
      "  Scraping livre 6/20...\n",
      "  Scraping livre 7/20...\n",
      "  Scraping livre 8/20...\n",
      "  Scraping livre 9/20...\n",
      "  Scraping livre 10/20...\n",
      "  Scraping livre 11/20...\n",
      "  Scraping livre 12/20...\n",
      "  Scraping livre 13/20...\n",
      "  Scraping livre 14/20...\n",
      "  Scraping livre 15/20...\n",
      "  Scraping livre 16/20...\n",
      "  Scraping livre 17/20...\n",
      "  Scraping livre 18/20...\n",
      "  Scraping livre 19/20...\n",
      "  Scraping livre 20/20...\n",
      "Scraping page 14...\n",
      "Trouvé 20 livres sur la page 14\n",
      "  Scraping livre 1/20...\n",
      "  Scraping livre 2/20...\n",
      "  Scraping livre 3/20...\n",
      "  Scraping livre 4/20...\n",
      "  Scraping livre 5/20...\n",
      "  Scraping livre 6/20...\n",
      "  Scraping livre 7/20...\n",
      "  Scraping livre 8/20...\n",
      "  Scraping livre 9/20...\n",
      "  Scraping livre 10/20...\n",
      "  Scraping livre 11/20...\n",
      "  Scraping livre 12/20...\n",
      "  Scraping livre 13/20...\n",
      "  Scraping livre 14/20...\n",
      "  Scraping livre 15/20...\n",
      "  Scraping livre 16/20...\n",
      "  Scraping livre 17/20...\n",
      "  Scraping livre 18/20...\n",
      "  Scraping livre 19/20...\n",
      "  Scraping livre 20/20...\n",
      "Scraping page 15...\n",
      "Trouvé 20 livres sur la page 15\n",
      "  Scraping livre 1/20...\n",
      "  Scraping livre 2/20...\n",
      "  Scraping livre 3/20...\n",
      "  Scraping livre 4/20...\n",
      "  Scraping livre 5/20...\n",
      "  Scraping livre 6/20...\n",
      "  Scraping livre 7/20...\n",
      "  Scraping livre 8/20...\n",
      "  Scraping livre 9/20...\n",
      "  Scraping livre 10/20...\n",
      "  Scraping livre 11/20...\n",
      "  Scraping livre 12/20...\n",
      "  Scraping livre 13/20...\n",
      "  Scraping livre 14/20...\n",
      "  Scraping livre 15/20...\n",
      "  Scraping livre 16/20...\n",
      "  Scraping livre 17/20...\n",
      "  Scraping livre 18/20...\n",
      "  Scraping livre 19/20...\n",
      "  Scraping livre 20/20...\n",
      "Scraping page 16...\n",
      "Trouvé 20 livres sur la page 16\n",
      "  Scraping livre 1/20...\n",
      "  Scraping livre 2/20...\n",
      "  Scraping livre 3/20...\n",
      "  Scraping livre 4/20...\n",
      "  Scraping livre 5/20...\n",
      "  Scraping livre 6/20...\n",
      "  Scraping livre 7/20...\n",
      "  Scraping livre 8/20...\n",
      "  Scraping livre 9/20...\n",
      "  Scraping livre 10/20...\n",
      "  Scraping livre 11/20...\n",
      "  Scraping livre 12/20...\n",
      "  Scraping livre 13/20...\n",
      "  Scraping livre 14/20...\n",
      "  Scraping livre 15/20...\n",
      "  Scraping livre 16/20...\n",
      "  Scraping livre 17/20...\n",
      "  Scraping livre 18/20...\n",
      "  Scraping livre 19/20...\n",
      "  Scraping livre 20/20...\n",
      "Scraping page 17...\n",
      "Trouvé 20 livres sur la page 17\n",
      "  Scraping livre 1/20...\n",
      "  Scraping livre 2/20...\n",
      "  Scraping livre 3/20...\n",
      "  Scraping livre 4/20...\n",
      "  Scraping livre 5/20...\n",
      "  Scraping livre 6/20...\n",
      "  Scraping livre 7/20...\n",
      "  Scraping livre 8/20...\n",
      "  Scraping livre 9/20...\n",
      "  Scraping livre 10/20...\n",
      "  Scraping livre 11/20...\n",
      "  Scraping livre 12/20...\n",
      "  Scraping livre 13/20...\n",
      "  Scraping livre 14/20...\n",
      "  Scraping livre 15/20...\n",
      "  Scraping livre 16/20...\n",
      "  Scraping livre 17/20...\n",
      "  Scraping livre 18/20...\n",
      "  Scraping livre 19/20...\n",
      "  Scraping livre 20/20...\n",
      "Scraping page 18...\n",
      "Trouvé 20 livres sur la page 18\n",
      "  Scraping livre 1/20...\n",
      "  Scraping livre 2/20...\n",
      "  Scraping livre 3/20...\n",
      "  Scraping livre 4/20...\n",
      "  Scraping livre 5/20...\n",
      "  Scraping livre 6/20...\n",
      "  Scraping livre 7/20...\n",
      "  Scraping livre 8/20...\n",
      "  Scraping livre 9/20...\n",
      "  Scraping livre 10/20...\n",
      "  Scraping livre 11/20...\n",
      "  Scraping livre 12/20...\n",
      "  Scraping livre 13/20...\n",
      "  Scraping livre 14/20...\n",
      "  Scraping livre 15/20...\n",
      "  Scraping livre 16/20...\n",
      "  Scraping livre 17/20...\n",
      "  Scraping livre 18/20...\n",
      "  Scraping livre 19/20...\n",
      "  Scraping livre 20/20...\n",
      "Scraping page 19...\n",
      "Trouvé 20 livres sur la page 19\n",
      "  Scraping livre 1/20...\n",
      "  Scraping livre 2/20...\n",
      "  Scraping livre 3/20...\n",
      "  Scraping livre 4/20...\n",
      "  Scraping livre 5/20...\n",
      "  Scraping livre 6/20...\n",
      "  Scraping livre 7/20...\n",
      "  Scraping livre 8/20...\n",
      "  Scraping livre 9/20...\n",
      "  Scraping livre 10/20...\n",
      "  Scraping livre 11/20...\n",
      "  Scraping livre 12/20...\n",
      "  Scraping livre 13/20...\n",
      "  Scraping livre 14/20...\n",
      "  Scraping livre 15/20...\n",
      "  Scraping livre 16/20...\n",
      "  Scraping livre 17/20...\n",
      "  Scraping livre 18/20...\n",
      "  Scraping livre 19/20...\n",
      "  Scraping livre 20/20...\n",
      "Scraping page 20...\n",
      "Trouvé 20 livres sur la page 20\n",
      "  Scraping livre 1/20...\n",
      "  Scraping livre 2/20...\n",
      "  Scraping livre 3/20...\n",
      "  Scraping livre 4/20...\n",
      "  Scraping livre 5/20...\n",
      "  Scraping livre 6/20...\n",
      "  Scraping livre 7/20...\n",
      "  Scraping livre 8/20...\n",
      "  Scraping livre 9/20...\n",
      "  Scraping livre 10/20...\n",
      "  Scraping livre 11/20...\n",
      "  Scraping livre 12/20...\n",
      "  Scraping livre 13/20...\n",
      "  Scraping livre 14/20...\n",
      "  Scraping livre 15/20...\n",
      "  Scraping livre 16/20...\n",
      "  Scraping livre 17/20...\n",
      "  Scraping livre 18/20...\n",
      "  Scraping livre 19/20...\n",
      "  Scraping livre 20/20...\n",
      "Scraping page 21...\n",
      "Trouvé 20 livres sur la page 21\n",
      "  Scraping livre 1/20...\n",
      "  Scraping livre 2/20...\n",
      "  Scraping livre 3/20...\n",
      "  Scraping livre 4/20...\n",
      "  Scraping livre 5/20...\n",
      "  Scraping livre 6/20...\n",
      "  Scraping livre 7/20...\n",
      "  Scraping livre 8/20...\n",
      "  Scraping livre 9/20...\n",
      "  Scraping livre 10/20...\n",
      "  Scraping livre 11/20...\n",
      "  Scraping livre 12/20...\n",
      "  Scraping livre 13/20...\n",
      "  Scraping livre 14/20...\n",
      "  Scraping livre 15/20...\n",
      "  Scraping livre 16/20...\n",
      "  Scraping livre 17/20...\n",
      "  Scraping livre 18/20...\n",
      "  Scraping livre 19/20...\n",
      "  Scraping livre 20/20...\n",
      "Scraping page 22...\n",
      "Trouvé 20 livres sur la page 22\n",
      "  Scraping livre 1/20...\n",
      "  Scraping livre 2/20...\n",
      "  Scraping livre 3/20...\n",
      "  Scraping livre 4/20...\n",
      "  Scraping livre 5/20...\n",
      "  Scraping livre 6/20...\n",
      "  Scraping livre 7/20...\n",
      "  Scraping livre 8/20...\n",
      "  Scraping livre 9/20...\n",
      "  Scraping livre 10/20...\n",
      "  Scraping livre 11/20...\n",
      "  Scraping livre 12/20...\n",
      "  Scraping livre 13/20...\n",
      "  Scraping livre 14/20...\n",
      "  Scraping livre 15/20...\n",
      "  Scraping livre 16/20...\n",
      "  Scraping livre 17/20...\n",
      "  Scraping livre 18/20...\n",
      "  Scraping livre 19/20...\n",
      "  Scraping livre 20/20...\n",
      "Scraping page 23...\n",
      "Trouvé 20 livres sur la page 23\n",
      "  Scraping livre 1/20...\n",
      "  Scraping livre 2/20...\n",
      "  Scraping livre 3/20...\n",
      "  Scraping livre 4/20...\n",
      "  Scraping livre 5/20...\n",
      "  Scraping livre 6/20...\n",
      "  Scraping livre 7/20...\n",
      "  Scraping livre 8/20...\n",
      "  Scraping livre 9/20...\n",
      "  Scraping livre 10/20...\n",
      "  Scraping livre 11/20...\n",
      "  Scraping livre 12/20...\n",
      "  Scraping livre 13/20...\n",
      "  Scraping livre 14/20...\n",
      "  Scraping livre 15/20...\n",
      "  Scraping livre 16/20...\n",
      "  Scraping livre 17/20...\n",
      "  Scraping livre 18/20...\n",
      "  Scraping livre 19/20...\n",
      "  Scraping livre 20/20...\n",
      "Scraping page 24...\n",
      "Trouvé 20 livres sur la page 24\n",
      "  Scraping livre 1/20...\n",
      "  Scraping livre 2/20...\n",
      "  Scraping livre 3/20...\n",
      "  Scraping livre 4/20...\n",
      "  Scraping livre 5/20...\n",
      "  Scraping livre 6/20...\n",
      "  Scraping livre 7/20...\n",
      "  Scraping livre 8/20...\n",
      "  Scraping livre 9/20...\n",
      "  Scraping livre 10/20...\n",
      "  Scraping livre 11/20...\n",
      "  Scraping livre 12/20...\n",
      "  Scraping livre 13/20...\n",
      "  Scraping livre 14/20...\n",
      "  Scraping livre 15/20...\n",
      "  Scraping livre 16/20...\n",
      "  Scraping livre 17/20...\n",
      "  Scraping livre 18/20...\n",
      "  Scraping livre 19/20...\n",
      "  Scraping livre 20/20...\n",
      "Scraping page 25...\n",
      "Trouvé 20 livres sur la page 25\n",
      "  Scraping livre 1/20...\n",
      "  Scraping livre 2/20...\n",
      "  Scraping livre 3/20...\n",
      "  Scraping livre 4/20...\n",
      "  Scraping livre 5/20...\n",
      "  Scraping livre 6/20...\n",
      "  Scraping livre 7/20...\n",
      "  Scraping livre 8/20...\n",
      "  Scraping livre 9/20...\n",
      "  Scraping livre 10/20...\n",
      "  Scraping livre 11/20...\n",
      "  Scraping livre 12/20...\n",
      "  Scraping livre 13/20...\n",
      "  Scraping livre 14/20...\n",
      "  Scraping livre 15/20...\n",
      "  Scraping livre 16/20...\n",
      "  Scraping livre 17/20...\n",
      "  Scraping livre 18/20...\n",
      "  Scraping livre 19/20...\n",
      "  Scraping livre 20/20...\n",
      "Scraping page 26...\n",
      "Trouvé 20 livres sur la page 26\n",
      "  Scraping livre 1/20...\n",
      "  Scraping livre 2/20...\n",
      "  Scraping livre 3/20...\n",
      "  Scraping livre 4/20...\n",
      "  Scraping livre 5/20...\n",
      "  Scraping livre 6/20...\n",
      "  Scraping livre 7/20...\n",
      "  Scraping livre 8/20...\n",
      "  Scraping livre 9/20...\n",
      "  Scraping livre 10/20...\n",
      "  Scraping livre 11/20...\n",
      "  Scraping livre 12/20...\n",
      "  Scraping livre 13/20...\n",
      "  Scraping livre 14/20...\n",
      "  Scraping livre 15/20...\n",
      "  Scraping livre 16/20...\n",
      "  Scraping livre 17/20...\n",
      "  Scraping livre 18/20...\n",
      "  Scraping livre 19/20...\n",
      "  Scraping livre 20/20...\n",
      "Scraping page 27...\n",
      "Trouvé 20 livres sur la page 27\n",
      "  Scraping livre 1/20...\n",
      "  Scraping livre 2/20...\n",
      "  Scraping livre 3/20...\n",
      "  Scraping livre 4/20...\n",
      "  Scraping livre 5/20...\n",
      "  Scraping livre 6/20...\n",
      "  Scraping livre 7/20...\n",
      "  Scraping livre 8/20...\n",
      "  Scraping livre 9/20...\n",
      "  Scraping livre 10/20...\n",
      "  Scraping livre 11/20...\n",
      "  Scraping livre 12/20...\n",
      "  Scraping livre 13/20...\n",
      "  Scraping livre 14/20...\n",
      "  Scraping livre 15/20...\n",
      "  Scraping livre 16/20...\n",
      "  Scraping livre 17/20...\n",
      "  Scraping livre 18/20...\n",
      "  Scraping livre 19/20...\n",
      "  Scraping livre 20/20...\n",
      "Scraping page 28...\n",
      "Trouvé 20 livres sur la page 28\n",
      "  Scraping livre 1/20...\n",
      "  Scraping livre 2/20...\n",
      "  Scraping livre 3/20...\n",
      "  Scraping livre 4/20...\n",
      "  Scraping livre 5/20...\n",
      "  Scraping livre 6/20...\n",
      "  Scraping livre 7/20...\n",
      "  Scraping livre 8/20...\n",
      "  Scraping livre 9/20...\n",
      "  Scraping livre 10/20...\n",
      "  Scraping livre 11/20...\n",
      "  Scraping livre 12/20...\n",
      "  Scraping livre 13/20...\n",
      "  Scraping livre 14/20...\n",
      "  Scraping livre 15/20...\n",
      "  Scraping livre 16/20...\n",
      "  Scraping livre 17/20...\n",
      "  Scraping livre 18/20...\n",
      "  Scraping livre 19/20...\n",
      "  Scraping livre 20/20...\n",
      "Scraping page 29...\n",
      "Trouvé 20 livres sur la page 29\n",
      "  Scraping livre 1/20...\n",
      "  Scraping livre 2/20...\n",
      "  Scraping livre 3/20...\n",
      "  Scraping livre 4/20...\n",
      "  Scraping livre 5/20...\n",
      "  Scraping livre 6/20...\n",
      "  Scraping livre 7/20...\n",
      "  Scraping livre 8/20...\n",
      "  Scraping livre 9/20...\n",
      "  Scraping livre 10/20...\n",
      "  Scraping livre 11/20...\n",
      "  Scraping livre 12/20...\n",
      "  Scraping livre 13/20...\n",
      "  Scraping livre 14/20...\n",
      "  Scraping livre 15/20...\n",
      "  Scraping livre 16/20...\n",
      "  Scraping livre 17/20...\n",
      "  Scraping livre 18/20...\n",
      "  Scraping livre 19/20...\n",
      "  Scraping livre 20/20...\n",
      "Scraping page 30...\n",
      "Trouvé 20 livres sur la page 30\n",
      "  Scraping livre 1/20...\n",
      "  Scraping livre 2/20...\n",
      "  Scraping livre 3/20...\n",
      "  Scraping livre 4/20...\n",
      "  Scraping livre 5/20...\n",
      "  Scraping livre 6/20...\n",
      "  Scraping livre 7/20...\n",
      "  Scraping livre 8/20...\n",
      "  Scraping livre 9/20...\n",
      "  Scraping livre 10/20...\n",
      "  Scraping livre 11/20...\n",
      "  Scraping livre 12/20...\n",
      "  Scraping livre 13/20...\n",
      "  Scraping livre 14/20...\n",
      "  Scraping livre 15/20...\n",
      "  Scraping livre 16/20...\n",
      "  Scraping livre 17/20...\n",
      "  Scraping livre 18/20...\n",
      "  Scraping livre 19/20...\n",
      "  Scraping livre 20/20...\n",
      "Scraping page 31...\n",
      "Trouvé 20 livres sur la page 31\n",
      "  Scraping livre 1/20...\n",
      "  Scraping livre 2/20...\n",
      "  Scraping livre 3/20...\n",
      "  Scraping livre 4/20...\n",
      "  Scraping livre 5/20...\n",
      "  Scraping livre 6/20...\n",
      "  Scraping livre 7/20...\n",
      "  Scraping livre 8/20...\n",
      "  Scraping livre 9/20...\n",
      "  Scraping livre 10/20...\n",
      "  Scraping livre 11/20...\n",
      "  Scraping livre 12/20...\n",
      "  Scraping livre 13/20...\n",
      "  Scraping livre 14/20...\n",
      "  Scraping livre 15/20...\n",
      "  Scraping livre 16/20...\n",
      "  Scraping livre 17/20...\n",
      "  Scraping livre 18/20...\n",
      "  Scraping livre 19/20...\n",
      "  Scraping livre 20/20...\n",
      "Scraping page 32...\n",
      "Trouvé 20 livres sur la page 32\n",
      "  Scraping livre 1/20...\n",
      "  Scraping livre 2/20...\n",
      "  Scraping livre 3/20...\n",
      "  Scraping livre 4/20...\n",
      "  Scraping livre 5/20...\n",
      "  Scraping livre 6/20...\n",
      "  Scraping livre 7/20...\n",
      "  Scraping livre 8/20...\n",
      "  Scraping livre 9/20...\n",
      "  Scraping livre 10/20...\n",
      "  Scraping livre 11/20...\n",
      "  Scraping livre 12/20...\n",
      "  Scraping livre 13/20...\n",
      "  Scraping livre 14/20...\n",
      "  Scraping livre 15/20...\n",
      "  Scraping livre 16/20...\n",
      "  Scraping livre 17/20...\n",
      "  Scraping livre 18/20...\n",
      "  Scraping livre 19/20...\n",
      "  Scraping livre 20/20...\n",
      "Scraping page 33...\n",
      "Trouvé 20 livres sur la page 33\n",
      "  Scraping livre 1/20...\n",
      "  Scraping livre 2/20...\n",
      "  Scraping livre 3/20...\n",
      "  Scraping livre 4/20...\n",
      "  Scraping livre 5/20...\n",
      "  Scraping livre 6/20...\n",
      "  Scraping livre 7/20...\n",
      "  Scraping livre 8/20...\n",
      "  Scraping livre 9/20...\n",
      "  Scraping livre 10/20...\n",
      "  Scraping livre 11/20...\n",
      "  Scraping livre 12/20...\n",
      "  Scraping livre 13/20...\n",
      "  Scraping livre 14/20...\n",
      "  Scraping livre 15/20...\n",
      "  Scraping livre 16/20...\n",
      "  Scraping livre 17/20...\n",
      "  Scraping livre 18/20...\n",
      "  Scraping livre 19/20...\n",
      "  Scraping livre 20/20...\n",
      "Scraping page 34...\n",
      "Trouvé 20 livres sur la page 34\n",
      "  Scraping livre 1/20...\n",
      "  Scraping livre 2/20...\n",
      "  Scraping livre 3/20...\n",
      "  Scraping livre 4/20...\n",
      "  Scraping livre 5/20...\n",
      "  Scraping livre 6/20...\n",
      "  Scraping livre 7/20...\n",
      "  Scraping livre 8/20...\n",
      "  Scraping livre 9/20...\n",
      "  Scraping livre 10/20...\n",
      "  Scraping livre 11/20...\n",
      "  Scraping livre 12/20...\n",
      "  Scraping livre 13/20...\n",
      "  Scraping livre 14/20...\n",
      "  Scraping livre 15/20...\n",
      "  Scraping livre 16/20...\n",
      "  Scraping livre 17/20...\n",
      "  Scraping livre 18/20...\n",
      "  Scraping livre 19/20...\n",
      "  Scraping livre 20/20...\n",
      "Scraping page 35...\n",
      "Trouvé 20 livres sur la page 35\n",
      "  Scraping livre 1/20...\n",
      "  Scraping livre 2/20...\n",
      "  Scraping livre 3/20...\n",
      "  Scraping livre 4/20...\n",
      "  Scraping livre 5/20...\n",
      "  Scraping livre 6/20...\n",
      "  Scraping livre 7/20...\n",
      "  Scraping livre 8/20...\n",
      "  Scraping livre 9/20...\n",
      "  Scraping livre 10/20...\n",
      "  Scraping livre 11/20...\n",
      "  Scraping livre 12/20...\n",
      "  Scraping livre 13/20...\n",
      "  Scraping livre 14/20...\n",
      "  Scraping livre 15/20...\n",
      "  Scraping livre 16/20...\n",
      "  Scraping livre 17/20...\n",
      "  Scraping livre 18/20...\n",
      "  Scraping livre 19/20...\n",
      "  Scraping livre 20/20...\n",
      "Scraping page 36...\n",
      "Trouvé 20 livres sur la page 36\n",
      "  Scraping livre 1/20...\n",
      "  Scraping livre 2/20...\n",
      "  Scraping livre 3/20...\n",
      "  Scraping livre 4/20...\n",
      "  Scraping livre 5/20...\n",
      "  Scraping livre 6/20...\n",
      "  Scraping livre 7/20...\n",
      "  Scraping livre 8/20...\n",
      "  Scraping livre 9/20...\n",
      "  Scraping livre 10/20...\n",
      "  Scraping livre 11/20...\n",
      "  Scraping livre 12/20...\n",
      "  Scraping livre 13/20...\n",
      "  Scraping livre 14/20...\n",
      "  Scraping livre 15/20...\n",
      "  Scraping livre 16/20...\n",
      "  Scraping livre 17/20...\n",
      "  Scraping livre 18/20...\n",
      "  Scraping livre 19/20...\n",
      "  Scraping livre 20/20...\n",
      "Scraping page 37...\n",
      "Trouvé 20 livres sur la page 37\n",
      "  Scraping livre 1/20...\n",
      "  Scraping livre 2/20...\n",
      "  Scraping livre 3/20...\n",
      "  Scraping livre 4/20...\n",
      "  Scraping livre 5/20...\n",
      "  Scraping livre 6/20...\n",
      "  Scraping livre 7/20...\n",
      "  Scraping livre 8/20...\n",
      "  Scraping livre 9/20...\n",
      "  Scraping livre 10/20...\n",
      "  Scraping livre 11/20...\n",
      "  Scraping livre 12/20...\n",
      "  Scraping livre 13/20...\n",
      "  Scraping livre 14/20...\n",
      "  Scraping livre 15/20...\n",
      "  Scraping livre 16/20...\n",
      "  Scraping livre 17/20...\n",
      "  Scraping livre 18/20...\n",
      "  Scraping livre 19/20...\n",
      "  Scraping livre 20/20...\n",
      "Scraping page 38...\n",
      "Trouvé 20 livres sur la page 38\n",
      "  Scraping livre 1/20...\n",
      "  Scraping livre 2/20...\n",
      "  Scraping livre 3/20...\n",
      "  Scraping livre 4/20...\n",
      "  Scraping livre 5/20...\n",
      "  Scraping livre 6/20...\n",
      "  Scraping livre 7/20...\n",
      "  Scraping livre 8/20...\n",
      "  Scraping livre 9/20...\n",
      "  Scraping livre 10/20...\n",
      "  Scraping livre 11/20...\n",
      "  Scraping livre 12/20...\n",
      "  Scraping livre 13/20...\n",
      "  Scraping livre 14/20...\n",
      "  Scraping livre 15/20...\n",
      "  Scraping livre 16/20...\n",
      "  Scraping livre 17/20...\n",
      "  Scraping livre 18/20...\n",
      "  Scraping livre 19/20...\n",
      "  Scraping livre 20/20...\n",
      "Scraping page 39...\n",
      "Trouvé 20 livres sur la page 39\n",
      "  Scraping livre 1/20...\n",
      "  Scraping livre 2/20...\n",
      "  Scraping livre 3/20...\n",
      "  Scraping livre 4/20...\n",
      "  Scraping livre 5/20...\n",
      "  Scraping livre 6/20...\n",
      "  Scraping livre 7/20...\n",
      "  Scraping livre 8/20...\n",
      "  Scraping livre 9/20...\n",
      "  Scraping livre 10/20...\n",
      "  Scraping livre 11/20...\n",
      "  Scraping livre 12/20...\n",
      "  Scraping livre 13/20...\n",
      "  Scraping livre 14/20...\n",
      "  Scraping livre 15/20...\n",
      "  Scraping livre 16/20...\n",
      "  Scraping livre 17/20...\n",
      "  Scraping livre 18/20...\n",
      "  Scraping livre 19/20...\n",
      "  Scraping livre 20/20...\n",
      "Scraping page 40...\n",
      "Trouvé 20 livres sur la page 40\n",
      "  Scraping livre 1/20...\n",
      "  Scraping livre 2/20...\n",
      "  Scraping livre 3/20...\n",
      "  Scraping livre 4/20...\n",
      "  Scraping livre 5/20...\n",
      "  Scraping livre 6/20...\n",
      "  Scraping livre 7/20...\n",
      "  Scraping livre 8/20...\n",
      "  Scraping livre 9/20...\n",
      "  Scraping livre 10/20...\n",
      "  Scraping livre 11/20...\n",
      "  Scraping livre 12/20...\n",
      "  Scraping livre 13/20...\n",
      "  Scraping livre 14/20...\n",
      "  Scraping livre 15/20...\n",
      "  Scraping livre 16/20...\n",
      "  Scraping livre 17/20...\n",
      "  Scraping livre 18/20...\n",
      "  Scraping livre 19/20...\n",
      "  Scraping livre 20/20...\n",
      "Scraping page 41...\n",
      "Trouvé 20 livres sur la page 41\n",
      "  Scraping livre 1/20...\n",
      "  Scraping livre 2/20...\n",
      "  Scraping livre 3/20...\n",
      "  Scraping livre 4/20...\n",
      "  Scraping livre 5/20...\n",
      "  Scraping livre 6/20...\n",
      "  Scraping livre 7/20...\n",
      "  Scraping livre 8/20...\n",
      "  Scraping livre 9/20...\n",
      "  Scraping livre 10/20...\n",
      "  Scraping livre 11/20...\n",
      "  Scraping livre 12/20...\n",
      "  Scraping livre 13/20...\n",
      "  Scraping livre 14/20...\n",
      "  Scraping livre 15/20...\n",
      "  Scraping livre 16/20...\n",
      "  Scraping livre 17/20...\n",
      "  Scraping livre 18/20...\n",
      "  Scraping livre 19/20...\n",
      "  Scraping livre 20/20...\n",
      "Scraping page 42...\n",
      "Trouvé 20 livres sur la page 42\n",
      "  Scraping livre 1/20...\n",
      "  Scraping livre 2/20...\n",
      "  Scraping livre 3/20...\n",
      "  Scraping livre 4/20...\n",
      "  Scraping livre 5/20...\n",
      "  Scraping livre 6/20...\n",
      "  Scraping livre 7/20...\n",
      "  Scraping livre 8/20...\n",
      "  Scraping livre 9/20...\n",
      "  Scraping livre 10/20...\n",
      "  Scraping livre 11/20...\n",
      "  Scraping livre 12/20...\n",
      "  Scraping livre 13/20...\n",
      "  Scraping livre 14/20...\n",
      "  Scraping livre 15/20...\n",
      "  Scraping livre 16/20...\n",
      "  Scraping livre 17/20...\n",
      "  Scraping livre 18/20...\n",
      "  Scraping livre 19/20...\n",
      "  Scraping livre 20/20...\n",
      "Scraping page 43...\n",
      "Trouvé 20 livres sur la page 43\n",
      "  Scraping livre 1/20...\n",
      "  Scraping livre 2/20...\n",
      "  Scraping livre 3/20...\n",
      "  Scraping livre 4/20...\n",
      "  Scraping livre 5/20...\n",
      "  Scraping livre 6/20...\n",
      "  Scraping livre 7/20...\n",
      "  Scraping livre 8/20...\n",
      "  Scraping livre 9/20...\n",
      "  Scraping livre 10/20...\n",
      "  Scraping livre 11/20...\n",
      "  Scraping livre 12/20...\n",
      "  Scraping livre 13/20...\n",
      "  Scraping livre 14/20...\n",
      "  Scraping livre 15/20...\n",
      "  Scraping livre 16/20...\n",
      "  Scraping livre 17/20...\n",
      "  Scraping livre 18/20...\n",
      "  Scraping livre 19/20...\n",
      "  Scraping livre 20/20...\n",
      "Scraping page 44...\n",
      "Trouvé 20 livres sur la page 44\n",
      "  Scraping livre 1/20...\n",
      "  Scraping livre 2/20...\n",
      "  Scraping livre 3/20...\n",
      "  Scraping livre 4/20...\n",
      "  Scraping livre 5/20...\n",
      "  Scraping livre 6/20...\n",
      "  Scraping livre 7/20...\n",
      "  Scraping livre 8/20...\n",
      "  Scraping livre 9/20...\n",
      "  Scraping livre 10/20...\n",
      "  Scraping livre 11/20...\n",
      "  Scraping livre 12/20...\n",
      "  Scraping livre 13/20...\n",
      "  Scraping livre 14/20...\n",
      "  Scraping livre 15/20...\n",
      "  Scraping livre 16/20...\n",
      "  Scraping livre 17/20...\n",
      "  Scraping livre 18/20...\n",
      "  Scraping livre 19/20...\n",
      "  Scraping livre 20/20...\n",
      "Scraping page 45...\n",
      "Trouvé 20 livres sur la page 45\n",
      "  Scraping livre 1/20...\n",
      "  Scraping livre 2/20...\n",
      "  Scraping livre 3/20...\n",
      "  Scraping livre 4/20...\n",
      "  Scraping livre 5/20...\n",
      "  Scraping livre 6/20...\n",
      "  Scraping livre 7/20...\n",
      "  Scraping livre 8/20...\n",
      "  Scraping livre 9/20...\n",
      "  Scraping livre 10/20...\n",
      "  Scraping livre 11/20...\n",
      "  Scraping livre 12/20...\n",
      "  Scraping livre 13/20...\n",
      "  Scraping livre 14/20...\n",
      "  Scraping livre 15/20...\n",
      "  Scraping livre 16/20...\n",
      "  Scraping livre 17/20...\n",
      "  Scraping livre 18/20...\n",
      "  Scraping livre 19/20...\n",
      "  Scraping livre 20/20...\n",
      "Scraping page 46...\n",
      "Trouvé 20 livres sur la page 46\n",
      "  Scraping livre 1/20...\n",
      "  Scraping livre 2/20...\n",
      "  Scraping livre 3/20...\n",
      "  Scraping livre 4/20...\n",
      "  Scraping livre 5/20...\n",
      "  Scraping livre 6/20...\n",
      "  Scraping livre 7/20...\n",
      "  Scraping livre 8/20...\n",
      "  Scraping livre 9/20...\n",
      "  Scraping livre 10/20...\n",
      "  Scraping livre 11/20...\n",
      "  Scraping livre 12/20...\n",
      "  Scraping livre 13/20...\n",
      "  Scraping livre 14/20...\n",
      "  Scraping livre 15/20...\n",
      "  Scraping livre 16/20...\n",
      "  Scraping livre 17/20...\n",
      "  Scraping livre 18/20...\n",
      "  Scraping livre 19/20...\n",
      "  Scraping livre 20/20...\n",
      "Scraping page 47...\n",
      "Trouvé 20 livres sur la page 47\n",
      "  Scraping livre 1/20...\n",
      "  Scraping livre 2/20...\n",
      "  Scraping livre 3/20...\n",
      "  Scraping livre 4/20...\n",
      "  Scraping livre 5/20...\n",
      "  Scraping livre 6/20...\n",
      "  Scraping livre 7/20...\n",
      "  Scraping livre 8/20...\n",
      "  Scraping livre 9/20...\n",
      "  Scraping livre 10/20...\n",
      "  Scraping livre 11/20...\n",
      "  Scraping livre 12/20...\n",
      "  Scraping livre 13/20...\n",
      "  Scraping livre 14/20...\n",
      "  Scraping livre 15/20...\n",
      "  Scraping livre 16/20...\n",
      "  Scraping livre 17/20...\n",
      "  Scraping livre 18/20...\n",
      "  Scraping livre 19/20...\n",
      "  Scraping livre 20/20...\n",
      "Scraping page 48...\n",
      "Trouvé 20 livres sur la page 48\n",
      "  Scraping livre 1/20...\n",
      "  Scraping livre 2/20...\n",
      "  Scraping livre 3/20...\n",
      "  Scraping livre 4/20...\n",
      "  Scraping livre 5/20...\n",
      "  Scraping livre 6/20...\n",
      "  Scraping livre 7/20...\n",
      "  Scraping livre 8/20...\n",
      "  Scraping livre 9/20...\n",
      "  Scraping livre 10/20...\n",
      "  Scraping livre 11/20...\n",
      "  Scraping livre 12/20...\n",
      "  Scraping livre 13/20...\n",
      "  Scraping livre 14/20...\n",
      "  Scraping livre 15/20...\n",
      "  Scraping livre 16/20...\n",
      "  Scraping livre 17/20...\n",
      "  Scraping livre 18/20...\n",
      "  Scraping livre 19/20...\n",
      "  Scraping livre 20/20...\n",
      "Scraping page 49...\n",
      "Trouvé 20 livres sur la page 49\n",
      "  Scraping livre 1/20...\n",
      "  Scraping livre 2/20...\n",
      "  Scraping livre 3/20...\n",
      "  Scraping livre 4/20...\n",
      "  Scraping livre 5/20...\n",
      "  Scraping livre 6/20...\n",
      "  Scraping livre 7/20...\n",
      "  Scraping livre 8/20...\n",
      "  Scraping livre 9/20...\n",
      "  Scraping livre 10/20...\n",
      "  Scraping livre 11/20...\n",
      "  Scraping livre 12/20...\n",
      "  Scraping livre 13/20...\n",
      "  Scraping livre 14/20...\n",
      "  Scraping livre 15/20...\n",
      "  Scraping livre 16/20...\n",
      "  Scraping livre 17/20...\n",
      "  Scraping livre 18/20...\n",
      "  Scraping livre 19/20...\n",
      "  Scraping livre 20/20...\n",
      "Scraping page 50...\n",
      "Trouvé 20 livres sur la page 50\n",
      "  Scraping livre 1/20...\n",
      "  Scraping livre 2/20...\n",
      "  Scraping livre 3/20...\n",
      "  Scraping livre 4/20...\n",
      "  Scraping livre 5/20...\n",
      "  Scraping livre 6/20...\n",
      "  Scraping livre 7/20...\n",
      "  Scraping livre 8/20...\n",
      "  Scraping livre 9/20...\n",
      "  Scraping livre 10/20...\n",
      "  Scraping livre 11/20...\n",
      "  Scraping livre 12/20...\n",
      "  Scraping livre 13/20...\n",
      "  Scraping livre 14/20...\n",
      "  Scraping livre 15/20...\n",
      "  Scraping livre 16/20...\n",
      "  Scraping livre 17/20...\n",
      "  Scraping livre 18/20...\n",
      "  Scraping livre 19/20...\n",
      "  Scraping livre 20/20...\n",
      "Scraping terminé. 1000 livres récupérés.\n",
      "Prétraitement des données...\n",
      "✅ Prétraitement terminé\n",
      "✅ Données sauvegardées dans livres_bruts.csv\n",
      "✅ Table 'livres' créée dans la base de données\n",
      "✅ Données sauvegardées dans PostgreSQL\n",
      "Création du modèle de recommandation...\n",
      "✅ Modèle de recommandation sauvegardé dans 'recommendation_model.pkl'\n",
      "\n",
      "📚 Recommandations pour 'A Light in the Attic':\n",
      "  1. Quarter Life Poetry: Poems for the Young, Broke and Hangry\n",
      "  2. salt.\n",
      "  3. Twenty Love Poems and a Song of Despair\n",
      "  4. You can't bury them all: Poems\n",
      "  5. Whole Lotta Creativity Going On: 60 Fun and Unusual Exercises to Awaken and Strengthen Your Creativity\n",
      "\n",
      "✅ Processus terminé avec succès!\n"
     ]
    }
   ],
   "source": [
    "import csv\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "import time\n",
    "import joblib\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "from selenium.webdriver.chrome.options import Options\n",
    "from selenium.common.exceptions import TimeoutException, NoSuchElementException\n",
    "from sqlalchemy import create_engine, Column, Integer, String, Float, Text\n",
    "from sqlalchemy.ext.declarative import declarative_base\n",
    "from sqlalchemy.orm import sessionmaker\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from urllib.parse import urljoin\n",
    "\n",
    "# -----------------------\n",
    "# Configuration\n",
    "# -----------------------\n",
    "DB_URI = \"postgresql+psycopg2://postgres:admin@localhost:5432/booksdb\"\n",
    "BASE_URL = \"https://books.toscrape.com\"\n",
    "CSV_FILE = \"livres_bruts.csv\"\n",
    "\n",
    "# Mapping des ratings\n",
    "RATING_MAP = {\n",
    "    \"One\": 1,\n",
    "    \"Two\": 2,\n",
    "    \"Three\": 3,\n",
    "    \"Four\": 4,\n",
    "    \"Five\": 5\n",
    "}\n",
    "\n",
    "# Configuration SQLAlchemy\n",
    "Base = declarative_base()\n",
    "\n",
    "class Livre(Base):\n",
    "    __tablename__ = 'livres'\n",
    "    \n",
    "    id = Column(Integer, primary_key=True, autoincrement=True)\n",
    "    titre = Column(String(500), nullable=False)\n",
    "    description = Column(Text)\n",
    "    prix = Column(Float)\n",
    "    disponibilite = Column(Integer)\n",
    "    image_url = Column(String(500))\n",
    "    note = Column(Integer)\n",
    "\n",
    "def setup_driver():\n",
    "    \"\"\"Configure et initialise le driver Selenium\"\"\"\n",
    "    chrome_options = Options()\n",
    "    chrome_options.add_argument(\"--headless\")  # Mode sans interface\n",
    "    chrome_options.add_argument(\"--no-sandbox\")\n",
    "    chrome_options.add_argument(\"--disable-dev-shm-usage\")\n",
    "    chrome_options.add_argument(\"--window-size=1920,1080\")\n",
    "    \n",
    "    driver = webdriver.Chrome(options=chrome_options)\n",
    "    driver.implicitly_wait(10)\n",
    "    return driver\n",
    "\n",
    "def extract_stock_number(stock_text):\n",
    "    \"\"\"Extrait le nombre de livres disponibles du texte de stock\"\"\"\n",
    "    if not stock_text:\n",
    "        return 0\n",
    "    \n",
    "    # Recherche d'un nombre dans le texte\n",
    "    match = re.search(r'(\\d+)', stock_text)\n",
    "    if match:\n",
    "        return int(match.group(1))\n",
    "    \n",
    "    # Si \"In stock\" sans nombre, considérer comme 1\n",
    "    if \"in stock\" in stock_text.lower():\n",
    "        return 1\n",
    "    \n",
    "    return 0\n",
    "\n",
    "def clean_description(description):\n",
    "    \"\"\"Nettoie la description du livre\"\"\"\n",
    "    if not description:\n",
    "        return \"\"\n",
    "    \n",
    "    # Supprimer les espaces multiples\n",
    "    description = re.sub(r'\\s+', ' ', description)\n",
    "    \n",
    "    # Supprimer les caractères spéciaux indésirables\n",
    "    description = re.sub(r'[^\\w\\s.,!?;:\\'-]', '', description)\n",
    "    \n",
    "    # Supprimer \"...more\" à la fin\n",
    "    description = re.sub(r'\\.\\.\\.more$', '', description)\n",
    "    \n",
    "    return description.strip()\n",
    "\n",
    "def scrape_book_details(driver, book_url):\n",
    "    \"\"\"Scrape les détails d'un livre spécifique\"\"\"\n",
    "    try:\n",
    "        driver.get(book_url)\n",
    "        wait = WebDriverWait(driver, 10)\n",
    "        \n",
    "        # Attendre que la page soit chargée\n",
    "        wait.until(EC.presence_of_element_located((By.TAG_NAME, \"h1\")))\n",
    "        \n",
    "        # Titre\n",
    "        titre = driver.find_element(By.TAG_NAME, \"h1\").text\n",
    "        \n",
    "        # Description\n",
    "        description = \"\"\n",
    "        try:\n",
    "            desc_element = driver.find_element(By.CSS_SELECTOR, \"#product_description + p\")\n",
    "            description = desc_element.text\n",
    "        except NoSuchElementException:\n",
    "            description = titre  # Utiliser le titre si pas de description\n",
    "        \n",
    "        # Prix\n",
    "        prix_text = driver.find_element(By.CSS_SELECTOR, \".price_color\").text\n",
    "        prix = float(re.sub(r'[^\\d.]', '', prix_text))\n",
    "        \n",
    "        # Disponibilité\n",
    "        stock_text = driver.find_element(By.CSS_SELECTOR, \".instock\").text\n",
    "        disponibilite = extract_stock_number(stock_text)\n",
    "        \n",
    "        # Image URL\n",
    "        img_element = driver.find_element(By.CSS_SELECTOR, \"#product_gallery img\")\n",
    "        image_url = urljoin(BASE_URL, img_element.get_attribute(\"src\"))\n",
    "        \n",
    "        # Note\n",
    "        note = 0\n",
    "        try:\n",
    "            rating_element = driver.find_element(By.CSS_SELECTOR, \".star-rating\")\n",
    "            rating_class = rating_element.get_attribute(\"class\")\n",
    "            for word in rating_class.split():\n",
    "                if word in RATING_MAP:\n",
    "                    note = RATING_MAP[word]\n",
    "                    break\n",
    "        except NoSuchElementException:\n",
    "            pass\n",
    "        \n",
    "        return {\n",
    "            'titre': titre,\n",
    "            'description': clean_description(description),\n",
    "            'prix': prix,\n",
    "            'disponibilite': disponibilite,\n",
    "            'image_url': image_url,\n",
    "            'note': note\n",
    "        }\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Erreur lors du scraping de {book_url}: {e}\")\n",
    "        return None\n",
    "\n",
    "def scrape_all_books():\n",
    "    \"\"\"Scrape tous les livres du site\"\"\"\n",
    "    driver = setup_driver()\n",
    "    livres = []\n",
    "    \n",
    "    try:\n",
    "        print(\"Début du scraping...\")\n",
    "        \n",
    "        page = 1\n",
    "        while True:\n",
    "            url = f\"{BASE_URL}/catalogue/page-{page}.html\"\n",
    "            print(f\"Scraping page {page}...\")\n",
    "            \n",
    "            driver.get(url)\n",
    "            \n",
    "            # Vérifier si la page existe\n",
    "            try:\n",
    "                WebDriverWait(driver, 5).until(\n",
    "                    EC.presence_of_element_located((By.CSS_SELECTOR, \".product_pod\"))\n",
    "                )\n",
    "            except TimeoutException:\n",
    "                print(f\"Page {page} non trouvée. Arrêt du scraping.\")\n",
    "                break\n",
    "            \n",
    "            # Récupérer tous les liens des livres sur cette page\n",
    "            book_links = []\n",
    "            book_elements = driver.find_elements(By.CSS_SELECTOR, \".product_pod h3 a\")\n",
    "            \n",
    "            for element in book_elements:\n",
    "                book_href = element.get_attribute(\"href\")\n",
    "                if book_href:\n",
    "                    book_links.append(book_href)\n",
    "            \n",
    "            print(f\"Trouvé {len(book_links)} livres sur la page {page}\")\n",
    "            \n",
    "            # Scraper chaque livre\n",
    "            for i, book_url in enumerate(book_links, 1):\n",
    "                print(f\"  Scraping livre {i}/{len(book_links)}...\")\n",
    "                book_data = scrape_book_details(driver, book_url)\n",
    "                \n",
    "                if book_data:\n",
    "                    livres.append(book_data)\n",
    "                \n",
    "                # Petite pause pour éviter de surcharger le serveur\n",
    "                time.sleep(0.5)\n",
    "            \n",
    "            page += 1\n",
    "            \n",
    "            # Limiter à 50 pages pour éviter une boucle infinie\n",
    "            if page > 50:\n",
    "                break\n",
    "    \n",
    "    finally:\n",
    "        driver.quit()\n",
    "    \n",
    "    print(f\"Scraping terminé. {len(livres)} livres récupérés.\")\n",
    "    return livres\n",
    "\n",
    "def save_to_csv(livres, filename):\n",
    "    \"\"\"Sauvegarde les données dans un fichier CSV\"\"\"\n",
    "    df = pd.DataFrame(livres)\n",
    "    df.to_csv(filename, index=False, encoding='utf-8')\n",
    "    print(f\"✅ Données sauvegardées dans {filename}\")\n",
    "    return df\n",
    "\n",
    "def create_database_table(engine):\n",
    "    \"\"\"Crée la table dans la base de données\"\"\"\n",
    "    Base.metadata.create_all(engine)\n",
    "    print(\"✅ Table 'livres' créée dans la base de données\")\n",
    "\n",
    "def save_to_database(df, engine):\n",
    "    \"\"\"Sauvegarde les données dans la base de données\"\"\"\n",
    "    try:\n",
    "        df.to_sql('livres', engine, if_exists='replace', index=False)\n",
    "        print(\"✅ Données sauvegardées dans PostgreSQL\")\n",
    "    except Exception as e:\n",
    "        print(f\"❌ Erreur lors de la sauvegarde dans PostgreSQL: {e}\")\n",
    "\n",
    "def preprocess_data(df):\n",
    "    \"\"\"Prétraite les données\"\"\"\n",
    "    print(\"Prétraitement des données...\")\n",
    "    \n",
    "    # 1. Nettoyer la description (déjà fait dans clean_description)\n",
    "    df['description'] = df['description'].fillna('')\n",
    "    \n",
    "    # 2. Prix déjà converti en float\n",
    "    \n",
    "    # 3. Disponibilité déjà convertie en int\n",
    "    \n",
    "    # 4. Note déjà extraite sous forme numérique\n",
    "    \n",
    "    # 5. Remplir les valeurs manquantes de description avec le titre\n",
    "    mask = (df['description'] == '') | df['description'].isna()\n",
    "    df.loc[mask, 'description'] = df.loc[mask, 'titre']\n",
    "    \n",
    "    print(\"✅ Prétraitement terminé\")\n",
    "    return df\n",
    "\n",
    "def create_recommendation_model(df):\n",
    "    \"\"\"Crée le modèle de recommandation basé sur la similarité cosinus\"\"\"\n",
    "    print(\"Création du modèle de recommandation...\")\n",
    "    \n",
    "    # Préparer les descriptions pour TF-IDF\n",
    "    descriptions = df['description'].fillna('').astype(str)\n",
    "    \n",
    "    # Appliquer TF-IDF\n",
    "    vectorizer = TfidfVectorizer(\n",
    "        max_features=5000,\n",
    "        stop_words='english',\n",
    "        lowercase=True,\n",
    "        ngram_range=(1, 2)\n",
    "    )\n",
    "    \n",
    "    tfidf_matrix = vectorizer.fit_transform(descriptions)\n",
    "    \n",
    "    # Calculer la matrice de similarité cosinus\n",
    "    similarity_matrix = cosine_similarity(tfidf_matrix)\n",
    "    \n",
    "    # Sauvegarder le modèle\n",
    "    model_data = {\n",
    "        'vectorizer': vectorizer,\n",
    "        'tfidf_matrix': tfidf_matrix,\n",
    "        'similarity_matrix': similarity_matrix,\n",
    "        'titles': df['titre'].tolist(),\n",
    "        'indices': pd.Series(df.index, index=df['titre']).to_dict()\n",
    "    }\n",
    "    \n",
    "    joblib.dump(model_data, 'recommendation_model.pkl')\n",
    "    print(\"✅ Modèle de recommandation sauvegardé dans 'recommendation_model.pkl'\")\n",
    "    \n",
    "    return model_data\n",
    "\n",
    "def get_recommendations(titre, model_data, n_recommendations=5):\n",
    "    \"\"\"Obtient des recommandations pour un livre donné\"\"\"\n",
    "    try:\n",
    "        # Obtenir l'index du livre\n",
    "        idx = model_data['indices'][titre]\n",
    "        \n",
    "        # Obtenir les scores de similarité\n",
    "        sim_scores = list(enumerate(model_data['similarity_matrix'][idx]))\n",
    "        \n",
    "        # Trier par score de similarité (décroissant)\n",
    "        sim_scores = sorted(sim_scores, key=lambda x: x[1], reverse=True)\n",
    "        \n",
    "        # Obtenir les indices des livres similaires (exclure le livre lui-même)\n",
    "        sim_indices = [i[0] for i in sim_scores[1:n_recommendations+1]]\n",
    "        \n",
    "        # Retourner les titres des livres recommandés\n",
    "        return [model_data['titles'][i] for i in sim_indices]\n",
    "        \n",
    "    except KeyError:\n",
    "        return f\"Livre '{titre}' non trouvé dans la base de données\"\n",
    "\n",
    "def main():\n",
    "    \"\"\"Fonction principale\"\"\"\n",
    "    print(\"=== SCRAPING BOOKS TO SCRAPE AVEC SELENIUM ===\\n\")\n",
    "    \n",
    "    # 1. Scraper les données\n",
    "    livres = scrape_all_books()\n",
    "    \n",
    "    if not livres:\n",
    "        print(\"❌ Aucune donnée récupérée. Arrêt du programme.\")\n",
    "        return\n",
    "    \n",
    "    # 2. Créer le DataFrame\n",
    "    df = pd.DataFrame(livres)\n",
    "    \n",
    "    # 3. Prétraiter les données\n",
    "    df = preprocess_data(df)\n",
    "    \n",
    "    # 4. Sauvegarder dans le CSV\n",
    "    save_to_csv(df, CSV_FILE)\n",
    "    \n",
    "    # 5. Sauvegarder dans la base de données\n",
    "    try:\n",
    "        engine = create_engine(DB_URI)\n",
    "        create_database_table(engine)\n",
    "        save_to_database(df, engine)\n",
    "    except Exception as e:\n",
    "        print(f\"❌ Erreur avec la base de données: {e}\")\n",
    "    \n",
    "    # 6. Créer le modèle de recommandation\n",
    "    model_data = create_recommendation_model(df)\n",
    "    \n",
    "    # 7. Exemple de recommandations\n",
    "    if len(df) > 0:\n",
    "        premier_livre = df.iloc[0]['titre']\n",
    "        recommendations = get_recommendations(premier_livre, model_data)\n",
    "        print(f\"\\n📚 Recommandations pour '{premier_livre}':\")\n",
    "        for i, rec in enumerate(recommendations, 1):\n",
    "            print(f\"  {i}. {rec}\")\n",
    "    \n",
    "    print(\"\\n✅ Processus terminé avec succès!\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "320b0dd7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Début du scraping...\n",
      "Scraping page 1/50...\n",
      "Scraping page 2/50...\n",
      "Scraping page 3/50...\n",
      "Scraping page 4/50...\n",
      "Scraping page 5/50...\n",
      "Scraping page 6/50...\n",
      "Scraping page 7/50...\n",
      "Scraping page 8/50...\n",
      "Scraping page 9/50...\n",
      "Scraping page 10/50...\n",
      "Scraping page 11/50...\n",
      "Scraping page 12/50...\n",
      "Scraping page 13/50...\n",
      "Scraping page 14/50...\n",
      "Scraping page 15/50...\n",
      "Scraping page 16/50...\n",
      "Scraping page 17/50...\n",
      "Scraping page 18/50...\n",
      "Scraping page 19/50...\n",
      "Scraping page 20/50...\n",
      "Scraping page 21/50...\n",
      "Scraping page 22/50...\n",
      "Scraping page 23/50...\n",
      "Scraping page 24/50...\n",
      "Scraping page 25/50...\n",
      "Scraping page 26/50...\n",
      "Scraping page 27/50...\n",
      "Scraping page 28/50...\n",
      "Scraping page 29/50...\n",
      "Scraping page 30/50...\n",
      "Scraping page 31/50...\n",
      "Scraping page 32/50...\n",
      "Scraping page 33/50...\n",
      "Scraping page 34/50...\n",
      "Scraping page 35/50...\n",
      "Scraping page 36/50...\n",
      "Scraping page 37/50...\n",
      "Scraping page 38/50...\n",
      "Scraping page 39/50...\n",
      "Scraping page 40/50...\n",
      "Scraping page 41/50...\n",
      "Scraping page 42/50...\n",
      "Scraping page 43/50...\n",
      "Scraping page 44/50...\n",
      "Scraping page 45/50...\n",
      "Scraping page 46/50...\n",
      "Scraping page 47/50...\n",
      "Scraping page 48/50...\n",
      "Scraping page 49/50...\n",
      "Scraping page 50/50...\n",
      "Scraping terminé. 1000 livres récupérés.\n",
      "\n",
      "Aperçu des données:\n",
      "                                   titre  \\\n",
      "0                   A Light in the Attic   \n",
      "1                     Tipping the Velvet   \n",
      "2                             Soumission   \n",
      "3                          Sharp Objects   \n",
      "4  Sapiens: A Brief History of Humankind   \n",
      "\n",
      "                                         description   prix  disponibilite  \\\n",
      "0  It's hard to imagine a world without A Light i...  51.77              0   \n",
      "1  \"Erotic and absorbing...Written with starling ...  53.74              0   \n",
      "2  Dans une France assez proche de la nôtre, un h...  50.10              0   \n",
      "3  WICKED above her hipbone, GIRL across her hear...  47.82              0   \n",
      "4  From a renowned historian comes a groundbreaki...  54.23              0   \n",
      "\n",
      "                                           image_url  note  \n",
      "0  https://books.toscrape.com/media/cache/2c/da/2...     3  \n",
      "1  https://books.toscrape.com/media/cache/26/0c/2...     1  \n",
      "2  https://books.toscrape.com/media/cache/3e/ef/3...     1  \n",
      "3  https://books.toscrape.com/media/cache/32/51/3...     4  \n",
      "4  https://books.toscrape.com/media/cache/be/a5/b...     5  \n",
      "\n",
      "Nombre de livres: 1000\n",
      "Colonnes: ['titre', 'description', 'prix', 'disponibilite', 'image_url', 'note']\n",
      "Types: titre             object\n",
      "description       object\n",
      "prix             float64\n",
      "disponibilite      int64\n",
      "image_url         object\n",
      "note               int64\n",
      "dtype: object\n",
      "✅ Données sauvegardées dans PostgreSQL avec to_sql()\n",
      "✅ Vérification: 1000 livres dans la base\n",
      "✅ Données également sauvegardées dans livres.csv\n"
     ]
    }
   ],
   "source": [
    "import csv\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "from sqlalchemy import create_engine\n",
    "from urllib.parse import urljoin\n",
    "\n",
    "# -----------------------\n",
    "# Config PostgreSQL\n",
    "# -----------------------\n",
    "DB_URI = \"postgresql+psycopg2://postgres:admin@localhost:5432/booksdb\"\n",
    "\n",
    "base_url = \"https://books.toscrape.com/catalogue/page-{}.html\"\n",
    "site_base = \"https://books.toscrape.com/\"\n",
    "livres = []\n",
    "\n",
    "rating_map = {\n",
    "    \"One\": 1,\n",
    "    \"Two\": 2,\n",
    "    \"Three\": 3,\n",
    "    \"Four\": 4,\n",
    "    \"Five\": 5\n",
    "}\n",
    "\n",
    "# ======= SCRAPING =======\n",
    "print(\"Début du scraping...\")\n",
    "for page in range(1, 51):\n",
    "    url = base_url.format(page)\n",
    "    print(f\"Scraping page {page}/50...\")\n",
    "    response = requests.get(url)\n",
    "    response.encoding = \"latin-1\"\n",
    "    \n",
    "    soup = BeautifulSoup(response.text, \"html.parser\")\n",
    "    articles = soup.find_all(\"article\", class_=\"product_pod\")\n",
    "    \n",
    "    for article in articles:\n",
    "        try:\n",
    "            # Titre\n",
    "            titre = article.h3.a[\"title\"]\n",
    "            titre = titre.encode(\"latin-1\", errors=\"ignore\").decode(\"utf-8\", errors=\"ignore\")\n",
    "            \n",
    "            # Prix\n",
    "            prix_str = article.find(\"p\", class_=\"price_color\").text.strip()\n",
    "            prix_str = prix_str.replace(\"Â\", \"\").replace(\"£\", \"\")\n",
    "            prix = float(prix_str)\n",
    "            \n",
    "            # Stock/Disponibilité\n",
    "            stock_text = article.find(\"p\", class_=\"instock availability\").text.strip()\n",
    "            # Extract number from \"In stock (22 available)\"\n",
    "            import re\n",
    "            stock_match = re.search(r'\\((\\d+)', stock_text)\n",
    "            disponibilite = int(stock_match.group(1)) if stock_match else 0\n",
    "            \n",
    "            # Image URL\n",
    "            img_url = article.find(\"img\")[\"src\"]\n",
    "            img_url = urljoin(site_base, img_url)\n",
    "            \n",
    "            # Rating\n",
    "            rating_class = article.find(\"p\", class_=\"star-rating\")[\"class\"][1]\n",
    "            note = rating_map.get(rating_class, 0)\n",
    "            \n",
    "            # --- Récupération de la description ---\n",
    "            detail_href = article.h3.a[\"href\"]\n",
    "            detail_url = urljoin(site_base + \"catalogue/\", detail_href)\n",
    "            detail_resp = requests.get(detail_url)\n",
    "            detail_resp.encoding = \"latin-1\"\n",
    "            detail_soup = BeautifulSoup(detail_resp.text, \"html.parser\")\n",
    "            \n",
    "            desc_tag = detail_soup.find(\"div\", id=\"product_description\")\n",
    "            if desc_tag:\n",
    "                description = desc_tag.find_next_sibling(\"p\").text.strip()\n",
    "                description = description.encode(\"latin-1\", errors=\"ignore\").decode(\"utf-8\", errors=\"ignore\")\n",
    "            else:\n",
    "                description = \"\"\n",
    "            \n",
    "            livres.append({\n",
    "                'titre': titre,\n",
    "                'description': description,\n",
    "                'prix': prix,\n",
    "                'disponibilite': disponibilite,\n",
    "                'image_url': img_url,\n",
    "                'note': note\n",
    "            })\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Erreur lors du scraping d'un article: {e}\")\n",
    "            continue\n",
    "\n",
    "print(f\"Scraping terminé. {len(livres)} livres récupérés.\")\n",
    "\n",
    "# ======= CHARGEMENT DIRECT EN DB =======\n",
    "try:\n",
    "    # Créer DataFrame directement\n",
    "    df = pd.DataFrame(livres)\n",
    "    \n",
    "    # Vérifier les données\n",
    "    print(\"\\nAperçu des données:\")\n",
    "    print(df.head())\n",
    "    print(f\"\\nNombre de livres: {len(df)}\")\n",
    "    print(f\"Colonnes: {list(df.columns)}\")\n",
    "    print(f\"Types: {df.dtypes}\")\n",
    "    \n",
    "    # Connexion PostgreSQL via SQLAlchemy\n",
    "    engine = create_engine(DB_URI)\n",
    "    \n",
    "    # Sauvegarde dans la table \"livres\" \n",
    "    df.to_sql(\"livres\", engine, if_exists=\"replace\", index=False)\n",
    "    \n",
    "    print(\"✅ Données sauvegardées dans PostgreSQL avec to_sql()\")\n",
    "    \n",
    "    # Vérification\n",
    "    verification_query = \"SELECT COUNT(*) as total FROM livres\"\n",
    "    result = pd.read_sql_query(verification_query, engine)\n",
    "    print(f\"✅ Vérification: {result['total'].iloc[0]} livres dans la base\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(\"❌ Erreur lors de la sauvegarde dans PostgreSQL :\", e)\n",
    "\n",
    "# ======= SAUVEGARDE CSV OPTIONNELLE =======\n",
    "try:\n",
    "    csv_file = \"livres.csv\"\n",
    "    df.to_csv(csv_file, index=False, encoding=\"utf-8\")\n",
    "    print(f\"✅ Données également sauvegardées dans {csv_file}\")\n",
    "except Exception as e:\n",
    "    print(f\"❌ Erreur sauvegarde CSV: {e}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
