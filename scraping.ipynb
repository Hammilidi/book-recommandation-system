{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e691f11-f1c1-44b0-8456-4d6fafab5fc8",
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install psycopg2 requests beautifulsoup4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d66db4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "from sqlalchemy import create_engine\n",
    "\n",
    "# -----------------------\n",
    "# Config PostgreSQL\n",
    "# -----------------------\n",
    "DB_URI = \"postgresql+psycopg2://postgres:admin@localhost:5432/booksdb\"\n",
    "\n",
    "base_url = \"https://books.toscrape.com/catalogue/page-{}.html\"\n",
    "livres = []\n",
    "\n",
    "rating_map = {\n",
    "    \"One\": \"1\",\n",
    "    \"Two\": \"2\",\n",
    "    \"Three\": \"3\",\n",
    "    \"Four\": \"4\",\n",
    "    \"Five\": \"5\"\n",
    "}\n",
    "\n",
    "# ======= SCRAPING =======\n",
    "for page in range(1, 51):\n",
    "    url = base_url.format(page)\n",
    "    response = requests.get(url)\n",
    "    response.encoding = \"latin-1\"\n",
    "    \n",
    "    soup = BeautifulSoup(response.text, \"html.parser\")\n",
    "    articles = soup.find_all(\"article\", class_=\"product_pod\")\n",
    "    \n",
    "    for article in articles:\n",
    "        titre = article.h3.a[\"title\"]\n",
    "        titre = titre.encode(\"latin-1\", errors=\"ignore\").decode(\"utf-8\", errors=\"ignore\")\n",
    "        \n",
    "        prix_str = article.find(\"p\", class_=\"price_color\").text.strip()\n",
    "        prix_str = prix_str.replace(\"Â\", \"\").replace(\"£\", \"\")\n",
    "        prix = float(prix_str)\n",
    "        \n",
    "        stock = article.find(\"p\", class_=\"instock availability\").text.strip()\n",
    "        stock = stock.encode(\"latin-1\", errors=\"ignore\").decode(\"utf-8\", errors=\"ignore\")\n",
    "        \n",
    "        img_url = article.find(\"img\")[\"src\"].replace(\"../../\", \"https://books.toscrape.com/\")\n",
    "        \n",
    "        rating_class = article.find(\"p\", class_=\"star-rating\")[\"class\"][1]\n",
    "        rating = int(rating_map.get(rating_class, \"0\"))\n",
    "        \n",
    "        livres.append([titre, prix, stock, img_url, rating])\n",
    "\n",
    "# Sauvegarde CSV en UTF-8\n",
    "csv_file = \"livres.csv\"\n",
    "with open(csv_file, \"w\", newline=\"\", encoding=\"utf-8\") as f:\n",
    "    writer = csv.writer(f)\n",
    "    writer.writerow([\"Titre\", \"Prix\", \"Stock\", \"Image URL\", \"Rating\"])\n",
    "    writer.writerows(livres)\n",
    "\n",
    "print(\"✅ Données sauvegardées dans livres.csv\")\n",
    "\n",
    "# ======= CHARGEMENT CSV & STOCKAGE DB =======\n",
    "try:\n",
    "    # Charger le CSV\n",
    "    df = pd.read_csv(csv_file, encoding=\"utf-8\")\n",
    "    \n",
    "    # Connexion PostgreSQL via SQLAlchemy\n",
    "    engine = create_engine(DB_URI)\n",
    "    \n",
    "    # Sauvegarde dans la table \"livres\"\n",
    "    df.to_sql(\"livres\", engine, if_exists=\"replace\", index=False)\n",
    "    \n",
    "    print(\"✅ Données sauvegardées dans PostgreSQL avec to_sql()\")\n",
    "except Exception as e:\n",
    "    print(\"❌ Erreur lors de la sauvegarde dans PostgreSQL :\", e)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80548c4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "from sqlalchemy import create_engine\n",
    "from urllib.parse import urljoin\n",
    "\n",
    "# -----------------------\n",
    "# Config PostgreSQL\n",
    "# -----------------------\n",
    "DB_URI = \"postgresql+psycopg2://postgres:admin@localhost:5432/booksdb\"\n",
    "\n",
    "base_url = \"https://books.toscrape.com/catalogue/page-{}.html\"\n",
    "site_base = \"https://books.toscrape.com/\"\n",
    "livres = []\n",
    "\n",
    "rating_map = {\n",
    "    \"One\": \"1\",\n",
    "    \"Two\": \"2\",\n",
    "    \"Three\": \"3\",\n",
    "    \"Four\": \"4\",\n",
    "    \"Five\": \"5\"\n",
    "}\n",
    "\n",
    "# ======= SCRAPING =======\n",
    "for page in range(1, 51):\n",
    "    url = base_url.format(page)\n",
    "    response = requests.get(url)\n",
    "    response.encoding = \"latin-1\"\n",
    "    \n",
    "    soup = BeautifulSoup(response.text, \"html.parser\")\n",
    "    articles = soup.find_all(\"article\", class_=\"product_pod\")\n",
    "    \n",
    "    for article in articles:\n",
    "        titre = article.h3.a[\"title\"]\n",
    "        titre = titre.encode(\"latin-1\", errors=\"ignore\").decode(\"utf-8\", errors=\"ignore\")\n",
    "        \n",
    "        prix_str = article.find(\"p\", class_=\"price_color\").text.strip()\n",
    "        prix_str = prix_str.replace(\"Â\", \"\").replace(\"£\", \"\")\n",
    "        prix = float(prix_str)\n",
    "        \n",
    "        stock = article.find(\"p\", class_=\"instock availability\").text.strip()\n",
    "        stock = stock.encode(\"latin-1\", errors=\"ignore\").decode(\"utf-8\", errors=\"ignore\")\n",
    "        \n",
    "        img_url = article.find(\"img\")[\"src\"]\n",
    "        img_url = urljoin(site_base, img_url)  # construit une URL absolue\n",
    "        \n",
    "        rating_class = article.find(\"p\", class_=\"star-rating\")[\"class\"][1]\n",
    "        rating = int(rating_map.get(rating_class, \"0\"))\n",
    "        \n",
    "        # --- Récupération de la description ---\n",
    "        detail_href = article.h3.a[\"href\"]\n",
    "        detail_url = urljoin(site_base + \"catalogue/\", detail_href)  # URL absolue\n",
    "        detail_resp = requests.get(detail_url)\n",
    "        detail_resp.encoding = \"latin-1\"\n",
    "        detail_soup = BeautifulSoup(detail_resp.text, \"html.parser\")\n",
    "        \n",
    "        desc_tag = detail_soup.find(\"div\", id=\"product_description\")\n",
    "        if desc_tag:\n",
    "            description = desc_tag.find_next_sibling(\"p\").text.strip()\n",
    "        else:\n",
    "            description = \"\"\n",
    "        \n",
    "        livres.append([titre, prix, stock, img_url, rating, description])\n",
    "\n",
    "# Sauvegarde CSV en UTF-8\n",
    "csv_file = \"livres.csv\"\n",
    "with open(csv_file, \"w\", newline=\"\", encoding=\"utf-8\") as f:\n",
    "    writer = csv.writer(f)\n",
    "    writer.writerow([\"Titre\", \"Prix\", \"Stock\", \"Image URL\", \"Rating\", \"Description\"])\n",
    "    writer.writerows(livres)\n",
    "\n",
    "print(\"✅ Données sauvegardées dans livres.csv\")\n",
    "\n",
    "# ======= CHARGEMENT CSV & STOCKAGE DB =======\n",
    "try:\n",
    "    # Charger le CSV\n",
    "    df = pd.read_csv(csv_file, encoding=\"utf-8\")\n",
    "    \n",
    "    # Connexion PostgreSQL via SQLAlchemy\n",
    "    engine = create_engine(DB_URI)\n",
    "    \n",
    "    # Sauvegarde dans la table \"livres\"\n",
    "    df.to_sql(\"livres\", engine, if_exists=\"replace\", index=False)\n",
    "    \n",
    "    print(\"✅ Données sauvegardées dans PostgreSQL avec to_sql()\")\n",
    "except Exception as e:\n",
    "    print(\"❌ Erreur lors de la sauvegarde dans PostgreSQL :\", e)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "180d76fb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5945c406",
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "import time\n",
    "import joblib\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "from selenium.webdriver.chrome.options import Options\n",
    "from selenium.common.exceptions import TimeoutException, NoSuchElementException\n",
    "from sqlalchemy import create_engine, Column, Integer, String, Float, Text\n",
    "from sqlalchemy.ext.declarative import declarative_base\n",
    "from sqlalchemy.orm import sessionmaker\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from urllib.parse import urljoin\n",
    "\n",
    "# -----------------------\n",
    "# Configuration\n",
    "# -----------------------\n",
    "DB_URI = \"postgresql+psycopg2://postgres:admin@localhost:5432/booksdb\"\n",
    "BASE_URL = \"https://books.toscrape.com\"\n",
    "CSV_FILE = \"livres_bruts.csv\"\n",
    "\n",
    "# Mapping des ratings\n",
    "RATING_MAP = {\n",
    "    \"One\": 1,\n",
    "    \"Two\": 2,\n",
    "    \"Three\": 3,\n",
    "    \"Four\": 4,\n",
    "    \"Five\": 5\n",
    "}\n",
    "\n",
    "# Configuration SQLAlchemy\n",
    "Base = declarative_base()\n",
    "\n",
    "class Livre(Base):\n",
    "    __tablename__ = 'livres'\n",
    "    \n",
    "    id = Column(Integer, primary_key=True, autoincrement=True)\n",
    "    titre = Column(String(500), nullable=False)\n",
    "    description = Column(Text)\n",
    "    prix = Column(Float)\n",
    "    disponibilite = Column(Integer)\n",
    "    image_url = Column(String(500))\n",
    "    note = Column(Integer)\n",
    "\n",
    "def setup_driver():\n",
    "    \"\"\"Configure et initialise le driver Selenium\"\"\"\n",
    "    chrome_options = Options()\n",
    "    chrome_options.add_argument(\"--headless\")  # Mode sans interface\n",
    "    chrome_options.add_argument(\"--no-sandbox\")\n",
    "    chrome_options.add_argument(\"--disable-dev-shm-usage\")\n",
    "    chrome_options.add_argument(\"--window-size=1920,1080\")\n",
    "    \n",
    "    driver = webdriver.Chrome(options=chrome_options)\n",
    "    driver.implicitly_wait(10)\n",
    "    return driver\n",
    "\n",
    "def extract_stock_number(stock_text):\n",
    "    \"\"\"Extrait le nombre de livres disponibles du texte de stock\"\"\"\n",
    "    if not stock_text:\n",
    "        return 0\n",
    "    \n",
    "    # Recherche d'un nombre dans le texte\n",
    "    match = re.search(r'(\\d+)', stock_text)\n",
    "    if match:\n",
    "        return int(match.group(1))\n",
    "    \n",
    "    # Si \"In stock\" sans nombre, considérer comme 1\n",
    "    if \"in stock\" in stock_text.lower():\n",
    "        return 1\n",
    "    \n",
    "    return 0\n",
    "\n",
    "def clean_description(description):\n",
    "    \"\"\"Nettoie la description du livre\"\"\"\n",
    "    if not description:\n",
    "        return \"\"\n",
    "    \n",
    "    # Supprimer les espaces multiples\n",
    "    description = re.sub(r'\\s+', ' ', description)\n",
    "    \n",
    "    # Supprimer les caractères spéciaux indésirables\n",
    "    description = re.sub(r'[^\\w\\s.,!?;:\\'-]', '', description)\n",
    "    \n",
    "    # Supprimer \"...more\" à la fin\n",
    "    description = re.sub(r'\\.\\.\\.more$', '', description)\n",
    "    \n",
    "    return description.strip()\n",
    "\n",
    "def scrape_book_details(driver, book_url):\n",
    "    \"\"\"Scrape les détails d'un livre spécifique\"\"\"\n",
    "    try:\n",
    "        driver.get(book_url)\n",
    "        wait = WebDriverWait(driver, 10)\n",
    "        \n",
    "        # Attendre que la page soit chargée\n",
    "        wait.until(EC.presence_of_element_located((By.TAG_NAME, \"h1\")))\n",
    "        \n",
    "        # Titre\n",
    "        titre = driver.find_element(By.TAG_NAME, \"h1\").text\n",
    "        \n",
    "        # Description\n",
    "        description = \"\"\n",
    "        try:\n",
    "            desc_element = driver.find_element(By.CSS_SELECTOR, \"#product_description + p\")\n",
    "            description = desc_element.text\n",
    "        except NoSuchElementException:\n",
    "            description = titre  # Utiliser le titre si pas de description\n",
    "        \n",
    "        # Prix\n",
    "        prix_text = driver.find_element(By.CSS_SELECTOR, \".price_color\").text\n",
    "        prix = float(re.sub(r'[^\\d.]', '', prix_text))\n",
    "        \n",
    "        # Disponibilité\n",
    "        stock_text = driver.find_element(By.CSS_SELECTOR, \".instock\").text\n",
    "        disponibilite = extract_stock_number(stock_text)\n",
    "        \n",
    "        # Image URL\n",
    "        img_element = driver.find_element(By.CSS_SELECTOR, \"#product_gallery img\")\n",
    "        image_url = urljoin(BASE_URL, img_element.get_attribute(\"src\"))\n",
    "        \n",
    "        # Note\n",
    "        note = 0\n",
    "        try:\n",
    "            rating_element = driver.find_element(By.CSS_SELECTOR, \".star-rating\")\n",
    "            rating_class = rating_element.get_attribute(\"class\")\n",
    "            for word in rating_class.split():\n",
    "                if word in RATING_MAP:\n",
    "                    note = RATING_MAP[word]\n",
    "                    break\n",
    "        except NoSuchElementException:\n",
    "            pass\n",
    "        \n",
    "        return {\n",
    "            'titre': titre,\n",
    "            'description': clean_description(description),\n",
    "            'prix': prix,\n",
    "            'disponibilite': disponibilite,\n",
    "            'image_url': image_url,\n",
    "            'note': note\n",
    "        }\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Erreur lors du scraping de {book_url}: {e}\")\n",
    "        return None\n",
    "\n",
    "def scrape_all_books():\n",
    "    \"\"\"Scrape tous les livres du site\"\"\"\n",
    "    driver = setup_driver()\n",
    "    livres = []\n",
    "    \n",
    "    try:\n",
    "        print(\"Début du scraping...\")\n",
    "        \n",
    "        page = 1\n",
    "        while True:\n",
    "            url = f\"{BASE_URL}/catalogue/page-{page}.html\"\n",
    "            print(f\"Scraping page {page}...\")\n",
    "            \n",
    "            driver.get(url)\n",
    "            \n",
    "            # Vérifier si la page existe\n",
    "            try:\n",
    "                WebDriverWait(driver, 5).until(\n",
    "                    EC.presence_of_element_located((By.CSS_SELECTOR, \".product_pod\"))\n",
    "                )\n",
    "            except TimeoutException:\n",
    "                print(f\"Page {page} non trouvée. Arrêt du scraping.\")\n",
    "                break\n",
    "            \n",
    "            # Récupérer tous les liens des livres sur cette page\n",
    "            book_links = []\n",
    "            book_elements = driver.find_elements(By.CSS_SELECTOR, \".product_pod h3 a\")\n",
    "            \n",
    "            for element in book_elements:\n",
    "                book_href = element.get_attribute(\"href\")\n",
    "                if book_href:\n",
    "                    book_links.append(book_href)\n",
    "            \n",
    "            print(f\"Trouvé {len(book_links)} livres sur la page {page}\")\n",
    "            \n",
    "            # Scraper chaque livre\n",
    "            for i, book_url in enumerate(book_links, 1):\n",
    "                print(f\"  Scraping livre {i}/{len(book_links)}...\")\n",
    "                book_data = scrape_book_details(driver, book_url)\n",
    "                \n",
    "                if book_data:\n",
    "                    livres.append(book_data)\n",
    "                \n",
    "                # Petite pause pour éviter de surcharger le serveur\n",
    "                time.sleep(0.5)\n",
    "            \n",
    "            page += 1\n",
    "            \n",
    "            # Limiter à 50 pages pour éviter une boucle infinie\n",
    "            if page > 50:\n",
    "                break\n",
    "    \n",
    "    finally:\n",
    "        driver.quit()\n",
    "    \n",
    "    print(f\"Scraping terminé. {len(livres)} livres récupérés.\")\n",
    "    return livres\n",
    "\n",
    "def save_to_csv(livres, filename):\n",
    "    \"\"\"Sauvegarde les données dans un fichier CSV\"\"\"\n",
    "    df = pd.DataFrame(livres)\n",
    "    df.to_csv(filename, index=False, encoding='utf-8')\n",
    "    print(f\"✅ Données sauvegardées dans {filename}\")\n",
    "    return df\n",
    "\n",
    "def create_database_table(engine):\n",
    "    \"\"\"Crée la table dans la base de données\"\"\"\n",
    "    Base.metadata.create_all(engine)\n",
    "    print(\"✅ Table 'livres' créée dans la base de données\")\n",
    "\n",
    "def save_to_database(df, engine):\n",
    "    \"\"\"Sauvegarde les données dans la base de données\"\"\"\n",
    "    try:\n",
    "        df.to_sql('livres', engine, if_exists='replace', index=False)\n",
    "        print(\"✅ Données sauvegardées dans PostgreSQL\")\n",
    "    except Exception as e:\n",
    "        print(f\"❌ Erreur lors de la sauvegarde dans PostgreSQL: {e}\")\n",
    "\n",
    "def preprocess_data(df):\n",
    "    \"\"\"Prétraite les données\"\"\"\n",
    "    print(\"Prétraitement des données...\")\n",
    "    \n",
    "    # 1. Nettoyer la description (déjà fait dans clean_description)\n",
    "    df['description'] = df['description'].fillna('')\n",
    "    \n",
    "    # 2. Prix déjà converti en float\n",
    "    \n",
    "    # 3. Disponibilité déjà convertie en int\n",
    "    \n",
    "    # 4. Note déjà extraite sous forme numérique\n",
    "    \n",
    "    # 5. Remplir les valeurs manquantes de description avec le titre\n",
    "    mask = (df['description'] == '') | df['description'].isna()\n",
    "    df.loc[mask, 'description'] = df.loc[mask, 'titre']\n",
    "    \n",
    "    print(\"✅ Prétraitement terminé\")\n",
    "    return df\n",
    "\n",
    "def create_recommendation_model(df):\n",
    "    \"\"\"Crée le modèle de recommandation basé sur la similarité cosinus\"\"\"\n",
    "    print(\"Création du modèle de recommandation...\")\n",
    "    \n",
    "    # Préparer les descriptions pour TF-IDF\n",
    "    descriptions = df['description'].fillna('').astype(str)\n",
    "    \n",
    "    # Appliquer TF-IDF\n",
    "    vectorizer = TfidfVectorizer(\n",
    "        max_features=5000,\n",
    "        stop_words='english',\n",
    "        lowercase=True,\n",
    "        ngram_range=(1, 2)\n",
    "    )\n",
    "    \n",
    "    tfidf_matrix = vectorizer.fit_transform(descriptions)\n",
    "    \n",
    "    # Calculer la matrice de similarité cosinus\n",
    "    similarity_matrix = cosine_similarity(tfidf_matrix)\n",
    "    \n",
    "    # Sauvegarder le modèle\n",
    "    model_data = {\n",
    "        'vectorizer': vectorizer,\n",
    "        'tfidf_matrix': tfidf_matrix,\n",
    "        'similarity_matrix': similarity_matrix,\n",
    "        'titles': df['titre'].tolist(),\n",
    "        'indices': pd.Series(df.index, index=df['titre']).to_dict()\n",
    "    }\n",
    "    \n",
    "    joblib.dump(model_data, 'recommendation_model.pkl')\n",
    "    print(\"✅ Modèle de recommandation sauvegardé dans 'recommendation_model.pkl'\")\n",
    "    \n",
    "    return model_data\n",
    "\n",
    "def get_recommendations(titre, model_data, n_recommendations=5):\n",
    "    \"\"\"Obtient des recommandations pour un livre donné\"\"\"\n",
    "    try:\n",
    "        # Obtenir l'index du livre\n",
    "        idx = model_data['indices'][titre]\n",
    "        \n",
    "        # Obtenir les scores de similarité\n",
    "        sim_scores = list(enumerate(model_data['similarity_matrix'][idx]))\n",
    "        \n",
    "        # Trier par score de similarité (décroissant)\n",
    "        sim_scores = sorted(sim_scores, key=lambda x: x[1], reverse=True)\n",
    "        \n",
    "        # Obtenir les indices des livres similaires (exclure le livre lui-même)\n",
    "        sim_indices = [i[0] for i in sim_scores[1:n_recommendations+1]]\n",
    "        \n",
    "        # Retourner les titres des livres recommandés\n",
    "        return [model_data['titles'][i] for i in sim_indices]\n",
    "        \n",
    "    except KeyError:\n",
    "        return f\"Livre '{titre}' non trouvé dans la base de données\"\n",
    "\n",
    "def main():\n",
    "    \"\"\"Fonction principale\"\"\"\n",
    "    print(\"=== SCRAPING BOOKS TO SCRAPE AVEC SELENIUM ===\\n\")\n",
    "    \n",
    "    # 1. Scraper les données\n",
    "    livres = scrape_all_books()\n",
    "    \n",
    "    if not livres:\n",
    "        print(\"❌ Aucune donnée récupérée. Arrêt du programme.\")\n",
    "        return\n",
    "    \n",
    "    # 2. Créer le DataFrame\n",
    "    df = pd.DataFrame(livres)\n",
    "    \n",
    "    # 3. Prétraiter les données\n",
    "    df = preprocess_data(df)\n",
    "    \n",
    "    # 4. Sauvegarder dans le CSV\n",
    "    save_to_csv(df, CSV_FILE)\n",
    "    \n",
    "    # 5. Sauvegarder dans la base de données\n",
    "    try:\n",
    "        engine = create_engine(DB_URI)\n",
    "        create_database_table(engine)\n",
    "        save_to_database(df, engine)\n",
    "    except Exception as e:\n",
    "        print(f\"❌ Erreur avec la base de données: {e}\")\n",
    "    \n",
    "    # 6. Créer le modèle de recommandation\n",
    "    model_data = create_recommendation_model(df)\n",
    "    \n",
    "    # 7. Exemple de recommandations\n",
    "    if len(df) > 0:\n",
    "        premier_livre = df.iloc[0]['titre']\n",
    "        recommendations = get_recommendations(premier_livre, model_data)\n",
    "        print(f\"\\n📚 Recommandations pour '{premier_livre}':\")\n",
    "        for i, rec in enumerate(recommendations, 1):\n",
    "            print(f\"  {i}. {rec}\")\n",
    "    \n",
    "    print(\"\\n✅ Processus terminé avec succès!\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "320b0dd7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Début du scraping...\n",
      "Scraping page 1/50...\n",
      "Scraping page 2/50...\n",
      "Scraping page 3/50...\n",
      "Scraping page 4/50...\n",
      "Scraping page 5/50...\n",
      "Scraping page 6/50...\n",
      "Scraping page 7/50...\n",
      "Scraping page 8/50...\n",
      "Scraping page 9/50...\n",
      "Scraping page 10/50...\n",
      "Scraping page 11/50...\n",
      "Scraping page 12/50...\n",
      "Scraping page 13/50...\n",
      "Scraping page 14/50...\n",
      "Scraping page 15/50...\n",
      "Scraping page 16/50...\n",
      "Scraping page 17/50...\n",
      "Scraping page 18/50...\n",
      "Scraping page 19/50...\n",
      "Scraping page 20/50...\n",
      "Scraping page 21/50...\n",
      "Scraping page 22/50...\n",
      "Scraping page 23/50...\n",
      "Scraping page 24/50...\n",
      "Scraping page 25/50...\n",
      "Scraping page 26/50...\n",
      "Scraping page 27/50...\n",
      "Scraping page 28/50...\n",
      "Scraping page 29/50...\n",
      "Scraping page 30/50...\n",
      "Scraping page 31/50...\n",
      "Scraping page 32/50...\n",
      "Scraping page 33/50...\n",
      "Scraping page 34/50...\n",
      "Scraping page 35/50...\n",
      "Scraping page 36/50...\n",
      "Scraping page 37/50...\n",
      "Scraping page 38/50...\n",
      "Scraping page 39/50...\n",
      "Scraping page 40/50...\n",
      "Scraping page 41/50...\n",
      "Scraping page 42/50...\n",
      "Scraping page 43/50...\n",
      "Scraping page 44/50...\n",
      "Scraping page 45/50...\n",
      "Scraping page 46/50...\n",
      "Scraping page 47/50...\n",
      "Scraping page 48/50...\n",
      "Scraping page 49/50...\n",
      "Scraping page 50/50...\n",
      "Scraping terminé. 1000 livres récupérés.\n",
      "\n",
      "Aperçu des données avec ID :\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>titre</th>\n",
       "      <th>description</th>\n",
       "      <th>prix</th>\n",
       "      <th>disponibilite</th>\n",
       "      <th>image_url</th>\n",
       "      <th>note</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>A Light in the Attic</td>\n",
       "      <td>It's hard to imagine a world without A Light i...</td>\n",
       "      <td>51.77</td>\n",
       "      <td>0</td>\n",
       "      <td>https://books.toscrape.com/media/cache/2c/da/2...</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>Tipping the Velvet</td>\n",
       "      <td>\"Erotic and absorbing...Written with starling ...</td>\n",
       "      <td>53.74</td>\n",
       "      <td>0</td>\n",
       "      <td>https://books.toscrape.com/media/cache/26/0c/2...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>Soumission</td>\n",
       "      <td>Dans une France assez proche de la nôtre, un h...</td>\n",
       "      <td>50.10</td>\n",
       "      <td>0</td>\n",
       "      <td>https://books.toscrape.com/media/cache/3e/ef/3...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>Sharp Objects</td>\n",
       "      <td>WICKED above her hipbone, GIRL across her hear...</td>\n",
       "      <td>47.82</td>\n",
       "      <td>0</td>\n",
       "      <td>https://books.toscrape.com/media/cache/32/51/3...</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>Sapiens: A Brief History of Humankind</td>\n",
       "      <td>From a renowned historian comes a groundbreaki...</td>\n",
       "      <td>54.23</td>\n",
       "      <td>0</td>\n",
       "      <td>https://books.toscrape.com/media/cache/be/a5/b...</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   id                                  titre  \\\n",
       "0   0                   A Light in the Attic   \n",
       "1   1                     Tipping the Velvet   \n",
       "2   2                             Soumission   \n",
       "3   3                          Sharp Objects   \n",
       "4   4  Sapiens: A Brief History of Humankind   \n",
       "\n",
       "                                         description   prix  disponibilite  \\\n",
       "0  It's hard to imagine a world without A Light i...  51.77              0   \n",
       "1  \"Erotic and absorbing...Written with starling ...  53.74              0   \n",
       "2  Dans une France assez proche de la nôtre, un h...  50.10              0   \n",
       "3  WICKED above her hipbone, GIRL across her hear...  47.82              0   \n",
       "4  From a renowned historian comes a groundbreaki...  54.23              0   \n",
       "\n",
       "                                           image_url  note  \n",
       "0  https://books.toscrape.com/media/cache/2c/da/2...     3  \n",
       "1  https://books.toscrape.com/media/cache/26/0c/2...     1  \n",
       "2  https://books.toscrape.com/media/cache/3e/ef/3...     1  \n",
       "3  https://books.toscrape.com/media/cache/32/51/3...     4  \n",
       "4  https://books.toscrape.com/media/cache/be/a5/b...     5  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Aperçu des données:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>titre</th>\n",
       "      <th>description</th>\n",
       "      <th>prix</th>\n",
       "      <th>disponibilite</th>\n",
       "      <th>image_url</th>\n",
       "      <th>note</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>A Light in the Attic</td>\n",
       "      <td>It's hard to imagine a world without A Light i...</td>\n",
       "      <td>51.77</td>\n",
       "      <td>0</td>\n",
       "      <td>https://books.toscrape.com/media/cache/2c/da/2...</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>Tipping the Velvet</td>\n",
       "      <td>\"Erotic and absorbing...Written with starling ...</td>\n",
       "      <td>53.74</td>\n",
       "      <td>0</td>\n",
       "      <td>https://books.toscrape.com/media/cache/26/0c/2...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>Soumission</td>\n",
       "      <td>Dans une France assez proche de la nôtre, un h...</td>\n",
       "      <td>50.10</td>\n",
       "      <td>0</td>\n",
       "      <td>https://books.toscrape.com/media/cache/3e/ef/3...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>Sharp Objects</td>\n",
       "      <td>WICKED above her hipbone, GIRL across her hear...</td>\n",
       "      <td>47.82</td>\n",
       "      <td>0</td>\n",
       "      <td>https://books.toscrape.com/media/cache/32/51/3...</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>Sapiens: A Brief History of Humankind</td>\n",
       "      <td>From a renowned historian comes a groundbreaki...</td>\n",
       "      <td>54.23</td>\n",
       "      <td>0</td>\n",
       "      <td>https://books.toscrape.com/media/cache/be/a5/b...</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   id                                  titre  \\\n",
       "0   0                   A Light in the Attic   \n",
       "1   1                     Tipping the Velvet   \n",
       "2   2                             Soumission   \n",
       "3   3                          Sharp Objects   \n",
       "4   4  Sapiens: A Brief History of Humankind   \n",
       "\n",
       "                                         description   prix  disponibilite  \\\n",
       "0  It's hard to imagine a world without A Light i...  51.77              0   \n",
       "1  \"Erotic and absorbing...Written with starling ...  53.74              0   \n",
       "2  Dans une France assez proche de la nôtre, un h...  50.10              0   \n",
       "3  WICKED above her hipbone, GIRL across her hear...  47.82              0   \n",
       "4  From a renowned historian comes a groundbreaki...  54.23              0   \n",
       "\n",
       "                                           image_url  note  \n",
       "0  https://books.toscrape.com/media/cache/2c/da/2...     3  \n",
       "1  https://books.toscrape.com/media/cache/26/0c/2...     1  \n",
       "2  https://books.toscrape.com/media/cache/3e/ef/3...     1  \n",
       "3  https://books.toscrape.com/media/cache/32/51/3...     4  \n",
       "4  https://books.toscrape.com/media/cache/be/a5/b...     5  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Nombre de livres: 1000\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\"Colonnes: ['id', 'titre', 'description', 'prix', 'disponibilite', 'image_url', 'note']\""
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "'Types: id                 int64\\ntitre             object\\ndescription       object\\nprix             float64\\ndisponibilite      int64\\nimage_url         object\\nnote               int64\\ndtype: object'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "❌ Erreur lors de la sauvegarde dans PostgreSQL : (psycopg2.errors.DependentObjectsStillExist) ERREUR:  n'a pas pu supprimer table livres car d'autres objets en dépendent\n",
      "DETAIL:  contrainte emprunts_id_livre_fkey sur table emprunts dépend de table livres\n",
      "contrainte reservations_id_livre_fkey sur table reservations dépend de table livres\n",
      "HINT:  Utilisez DROP ... CASCADE pour supprimer aussi les objets dépendants.\n",
      "\n",
      "[SQL: \n",
      "DROP TABLE livres]\n",
      "(Background on this error at: https://sqlalche.me/e/20/2j85)\n",
      "✅ Données également sauvegardées dans livres.csv\n"
     ]
    }
   ],
   "source": [
    "import csv\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "from sqlalchemy import create_engine\n",
    "from urllib.parse import urljoin\n",
    "\n",
    "# -----------------------\n",
    "# Config PostgreSQL\n",
    "# -----------------------\n",
    "DB_URI = \"postgresql+psycopg2://postgres:admin@localhost:5432/booksdb\"\n",
    "\n",
    "base_url = \"https://books.toscrape.com/catalogue/page-{}.html\"\n",
    "site_base = \"https://books.toscrape.com/\"\n",
    "livres = []\n",
    "\n",
    "rating_map = {\n",
    "    \"One\": 1,\n",
    "    \"Two\": 2,\n",
    "    \"Three\": 3,\n",
    "    \"Four\": 4,\n",
    "    \"Five\": 5\n",
    "}\n",
    "\n",
    "# ======= SCRAPING =======\n",
    "print(\"Début du scraping...\")\n",
    "for page in range(1, 51):\n",
    "    url = base_url.format(page)\n",
    "    print(f\"Scraping page {page}/50...\")\n",
    "    response = requests.get(url)\n",
    "    response.encoding = \"latin-1\"\n",
    "    \n",
    "    soup = BeautifulSoup(response.text, \"html.parser\")\n",
    "    articles = soup.find_all(\"article\", class_=\"product_pod\")\n",
    "    \n",
    "    for article in articles:\n",
    "        try:\n",
    "            # Titre\n",
    "            titre = article.h3.a[\"title\"]\n",
    "            titre = titre.encode(\"latin-1\", errors=\"ignore\").decode(\"utf-8\", errors=\"ignore\")\n",
    "            \n",
    "            # Prix\n",
    "            prix_str = article.find(\"p\", class_=\"price_color\").text.strip()\n",
    "            prix_str = prix_str.replace(\"Â\", \"\").replace(\"£\", \"\")\n",
    "            prix = float(prix_str)\n",
    "            \n",
    "            # Stock/Disponibilité\n",
    "            stock_text = article.find(\"p\", class_=\"instock availability\").text.strip()\n",
    "            # Extract number from \"In stock (22 available)\"\n",
    "            import re\n",
    "            stock_match = re.search(r'\\((\\d+)', stock_text)\n",
    "            disponibilite = int(stock_match.group(1)) if stock_match else 0\n",
    "            \n",
    "            # Image URL\n",
    "            img_url = article.find(\"img\")[\"src\"]\n",
    "            img_url = urljoin(site_base, img_url)\n",
    "            \n",
    "            # Rating\n",
    "            rating_class = article.find(\"p\", class_=\"star-rating\")[\"class\"][1]\n",
    "            note = rating_map.get(rating_class, 0)\n",
    "            \n",
    "            # --- Récupération de la description ---\n",
    "            detail_href = article.h3.a[\"href\"]\n",
    "            detail_url = urljoin(site_base + \"catalogue/\", detail_href)\n",
    "            detail_resp = requests.get(detail_url)\n",
    "            detail_resp.encoding = \"latin-1\"\n",
    "            detail_soup = BeautifulSoup(detail_resp.text, \"html.parser\")\n",
    "            \n",
    "            desc_tag = detail_soup.find(\"div\", id=\"product_description\")\n",
    "            if desc_tag:\n",
    "                description = desc_tag.find_next_sibling(\"p\").text.strip()\n",
    "                description = description.encode(\"latin-1\", errors=\"ignore\").decode(\"utf-8\", errors=\"ignore\")\n",
    "            else:\n",
    "                description = \"\"\n",
    "            \n",
    "            livres.append({\n",
    "                'titre': titre,\n",
    "                'description': description,\n",
    "                'prix': prix,\n",
    "                'disponibilite': disponibilite,\n",
    "                'image_url': img_url,\n",
    "                'note': note\n",
    "            })\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Erreur lors du scraping d'un article: {e}\")\n",
    "            continue\n",
    "\n",
    "print(f\"Scraping terminé. {len(livres)} livres récupérés.\")\n",
    "\n",
    "# ======= CHARGEMENT DIRECT EN DB =======\n",
    "try:\n",
    "    # Créer DataFrame directement\n",
    "    df = pd.DataFrame(livres)\n",
    "    \n",
    "\n",
    "    # Ajouter une colonne 'id' à partir de l'index\n",
    "    df.reset_index(inplace=True)\n",
    "    df.rename(columns={'index': 'id'}, inplace=True)\n",
    "\n",
    "    # Vérifier les données\n",
    "    print(\"\\nAperçu des données avec ID :\")\n",
    "    display(df.head())\n",
    "\n",
    "    \n",
    "    # Vérifier les données\n",
    "    print(\"\\nAperçu des données:\")\n",
    "    display(df.head())\n",
    "    print(f\"\\nNombre de livres: {len(df)}\")\n",
    "    display(f\"Colonnes: {list(df.columns)}\")\n",
    "    display(f\"Types: {df.dtypes}\")\n",
    "    \n",
    "    # Connexion PostgreSQL via SQLAlchemy\n",
    "    engine = create_engine(DB_URI)\n",
    "    \n",
    "    # Sauvegarde dans la table \"livres\" \n",
    "    df.to_sql(\"livres\", engine, if_exists=\"replace\", index=False)\n",
    "    \n",
    "    print(\"✅ Données sauvegardées dans PostgreSQL avec to_sql()\")\n",
    "    \n",
    "    # Vérification\n",
    "    verification_query = \"SELECT COUNT(*) as total FROM livres\"\n",
    "    result = pd.read_sql_query(verification_query, engine)\n",
    "    print(f\"✅ Vérification: {result['total'].iloc[0]} livres dans la base\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(\"❌ Erreur lors de la sauvegarde dans PostgreSQL :\", e)\n",
    "\n",
    "# ======= SAUVEGARDE CSV OPTIONNELLE =======\n",
    "try:\n",
    "    csv_file = \"livres.csv\"\n",
    "    df.to_csv(csv_file, index=False, encoding=\"utf-8\")\n",
    "    print(f\"✅ Données également sauvegardées dans {csv_file}\")\n",
    "except Exception as e:\n",
    "    print(f\"❌ Erreur sauvegarde CSV: {e}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
