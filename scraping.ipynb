{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e691f11-f1c1-44b0-8456-4d6fafab5fc8",
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install psycopg2 requests beautifulsoup4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d66db4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "from sqlalchemy import create_engine\n",
    "\n",
    "# -----------------------\n",
    "# Config PostgreSQL\n",
    "# -----------------------\n",
    "DB_URI = \"postgresql+psycopg2://postgres:admin@localhost:5432/booksdb\"\n",
    "\n",
    "base_url = \"https://books.toscrape.com/catalogue/page-{}.html\"\n",
    "livres = []\n",
    "\n",
    "rating_map = {\n",
    "    \"One\": \"1\",\n",
    "    \"Two\": \"2\",\n",
    "    \"Three\": \"3\",\n",
    "    \"Four\": \"4\",\n",
    "    \"Five\": \"5\"\n",
    "}\n",
    "\n",
    "# ======= SCRAPING =======\n",
    "for page in range(1, 51):\n",
    "    url = base_url.format(page)\n",
    "    response = requests.get(url)\n",
    "    response.encoding = \"latin-1\"\n",
    "    \n",
    "    soup = BeautifulSoup(response.text, \"html.parser\")\n",
    "    articles = soup.find_all(\"article\", class_=\"product_pod\")\n",
    "    \n",
    "    for article in articles:\n",
    "        titre = article.h3.a[\"title\"]\n",
    "        titre = titre.encode(\"latin-1\", errors=\"ignore\").decode(\"utf-8\", errors=\"ignore\")\n",
    "        \n",
    "        prix_str = article.find(\"p\", class_=\"price_color\").text.strip()\n",
    "        prix_str = prix_str.replace(\"√Ç\", \"\").replace(\"¬£\", \"\")\n",
    "        prix = float(prix_str)\n",
    "        \n",
    "        stock = article.find(\"p\", class_=\"instock availability\").text.strip()\n",
    "        stock = stock.encode(\"latin-1\", errors=\"ignore\").decode(\"utf-8\", errors=\"ignore\")\n",
    "        \n",
    "        img_url = article.find(\"img\")[\"src\"].replace(\"../../\", \"https://books.toscrape.com/\")\n",
    "        \n",
    "        rating_class = article.find(\"p\", class_=\"star-rating\")[\"class\"][1]\n",
    "        rating = int(rating_map.get(rating_class, \"0\"))\n",
    "        \n",
    "        livres.append([titre, prix, stock, img_url, rating])\n",
    "\n",
    "# Sauvegarde CSV en UTF-8\n",
    "csv_file = \"livres.csv\"\n",
    "with open(csv_file, \"w\", newline=\"\", encoding=\"utf-8\") as f:\n",
    "    writer = csv.writer(f)\n",
    "    writer.writerow([\"Titre\", \"Prix\", \"Stock\", \"Image URL\", \"Rating\"])\n",
    "    writer.writerows(livres)\n",
    "\n",
    "print(\"‚úÖ Donn√©es sauvegard√©es dans livres.csv\")\n",
    "\n",
    "# ======= CHARGEMENT CSV & STOCKAGE DB =======\n",
    "try:\n",
    "    # Charger le CSV\n",
    "    df = pd.read_csv(csv_file, encoding=\"utf-8\")\n",
    "    \n",
    "    # Connexion PostgreSQL via SQLAlchemy\n",
    "    engine = create_engine(DB_URI)\n",
    "    \n",
    "    # Sauvegarde dans la table \"livres\"\n",
    "    df.to_sql(\"livres\", engine, if_exists=\"replace\", index=False)\n",
    "    \n",
    "    print(\"‚úÖ Donn√©es sauvegard√©es dans PostgreSQL avec to_sql()\")\n",
    "except Exception as e:\n",
    "    print(\"‚ùå Erreur lors de la sauvegarde dans PostgreSQL :\", e)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80548c4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "from sqlalchemy import create_engine\n",
    "from urllib.parse import urljoin\n",
    "\n",
    "# -----------------------\n",
    "# Config PostgreSQL\n",
    "# -----------------------\n",
    "DB_URI = \"postgresql+psycopg2://postgres:admin@localhost:5432/booksdb\"\n",
    "\n",
    "base_url = \"https://books.toscrape.com/catalogue/page-{}.html\"\n",
    "site_base = \"https://books.toscrape.com/\"\n",
    "livres = []\n",
    "\n",
    "rating_map = {\n",
    "    \"One\": \"1\",\n",
    "    \"Two\": \"2\",\n",
    "    \"Three\": \"3\",\n",
    "    \"Four\": \"4\",\n",
    "    \"Five\": \"5\"\n",
    "}\n",
    "\n",
    "# ======= SCRAPING =======\n",
    "for page in range(1, 51):\n",
    "    url = base_url.format(page)\n",
    "    response = requests.get(url)\n",
    "    response.encoding = \"latin-1\"\n",
    "    \n",
    "    soup = BeautifulSoup(response.text, \"html.parser\")\n",
    "    articles = soup.find_all(\"article\", class_=\"product_pod\")\n",
    "    \n",
    "    for article in articles:\n",
    "        titre = article.h3.a[\"title\"]\n",
    "        titre = titre.encode(\"latin-1\", errors=\"ignore\").decode(\"utf-8\", errors=\"ignore\")\n",
    "        \n",
    "        prix_str = article.find(\"p\", class_=\"price_color\").text.strip()\n",
    "        prix_str = prix_str.replace(\"√Ç\", \"\").replace(\"¬£\", \"\")\n",
    "        prix = float(prix_str)\n",
    "        \n",
    "        stock = article.find(\"p\", class_=\"instock availability\").text.strip()\n",
    "        stock = stock.encode(\"latin-1\", errors=\"ignore\").decode(\"utf-8\", errors=\"ignore\")\n",
    "        \n",
    "        img_url = article.find(\"img\")[\"src\"]\n",
    "        img_url = urljoin(site_base, img_url)  # construit une URL absolue\n",
    "        \n",
    "        rating_class = article.find(\"p\", class_=\"star-rating\")[\"class\"][1]\n",
    "        rating = int(rating_map.get(rating_class, \"0\"))\n",
    "        \n",
    "        # --- R√©cup√©ration de la description ---\n",
    "        detail_href = article.h3.a[\"href\"]\n",
    "        detail_url = urljoin(site_base + \"catalogue/\", detail_href)  # URL absolue\n",
    "        detail_resp = requests.get(detail_url)\n",
    "        detail_resp.encoding = \"latin-1\"\n",
    "        detail_soup = BeautifulSoup(detail_resp.text, \"html.parser\")\n",
    "        \n",
    "        desc_tag = detail_soup.find(\"div\", id=\"product_description\")\n",
    "        if desc_tag:\n",
    "            description = desc_tag.find_next_sibling(\"p\").text.strip()\n",
    "        else:\n",
    "            description = \"\"\n",
    "        \n",
    "        livres.append([titre, prix, stock, img_url, rating, description])\n",
    "\n",
    "# Sauvegarde CSV en UTF-8\n",
    "csv_file = \"livres.csv\"\n",
    "with open(csv_file, \"w\", newline=\"\", encoding=\"utf-8\") as f:\n",
    "    writer = csv.writer(f)\n",
    "    writer.writerow([\"Titre\", \"Prix\", \"Stock\", \"Image URL\", \"Rating\", \"Description\"])\n",
    "    writer.writerows(livres)\n",
    "\n",
    "print(\"‚úÖ Donn√©es sauvegard√©es dans livres.csv\")\n",
    "\n",
    "# ======= CHARGEMENT CSV & STOCKAGE DB =======\n",
    "try:\n",
    "    # Charger le CSV\n",
    "    df = pd.read_csv(csv_file, encoding=\"utf-8\")\n",
    "    \n",
    "    # Connexion PostgreSQL via SQLAlchemy\n",
    "    engine = create_engine(DB_URI)\n",
    "    \n",
    "    # Sauvegarde dans la table \"livres\"\n",
    "    df.to_sql(\"livres\", engine, if_exists=\"replace\", index=False)\n",
    "    \n",
    "    print(\"‚úÖ Donn√©es sauvegard√©es dans PostgreSQL avec to_sql()\")\n",
    "except Exception as e:\n",
    "    print(\"‚ùå Erreur lors de la sauvegarde dans PostgreSQL :\", e)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "180d76fb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5945c406",
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "import time\n",
    "import joblib\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "from selenium.webdriver.chrome.options import Options\n",
    "from selenium.common.exceptions import TimeoutException, NoSuchElementException\n",
    "from sqlalchemy import create_engine, Column, Integer, String, Float, Text\n",
    "from sqlalchemy.ext.declarative import declarative_base\n",
    "from sqlalchemy.orm import sessionmaker\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from urllib.parse import urljoin\n",
    "\n",
    "# -----------------------\n",
    "# Configuration\n",
    "# -----------------------\n",
    "DB_URI = \"postgresql+psycopg2://postgres:admin@localhost:5432/booksdb\"\n",
    "BASE_URL = \"https://books.toscrape.com\"\n",
    "CSV_FILE = \"livres_bruts.csv\"\n",
    "\n",
    "# Mapping des ratings\n",
    "RATING_MAP = {\n",
    "    \"One\": 1,\n",
    "    \"Two\": 2,\n",
    "    \"Three\": 3,\n",
    "    \"Four\": 4,\n",
    "    \"Five\": 5\n",
    "}\n",
    "\n",
    "# Configuration SQLAlchemy\n",
    "Base = declarative_base()\n",
    "\n",
    "class Livre(Base):\n",
    "    __tablename__ = 'livres'\n",
    "    \n",
    "    id = Column(Integer, primary_key=True, autoincrement=True)\n",
    "    titre = Column(String(500), nullable=False)\n",
    "    description = Column(Text)\n",
    "    prix = Column(Float)\n",
    "    disponibilite = Column(Integer)\n",
    "    image_url = Column(String(500))\n",
    "    note = Column(Integer)\n",
    "\n",
    "def setup_driver():\n",
    "    \"\"\"Configure et initialise le driver Selenium\"\"\"\n",
    "    chrome_options = Options()\n",
    "    chrome_options.add_argument(\"--headless\")  # Mode sans interface\n",
    "    chrome_options.add_argument(\"--no-sandbox\")\n",
    "    chrome_options.add_argument(\"--disable-dev-shm-usage\")\n",
    "    chrome_options.add_argument(\"--window-size=1920,1080\")\n",
    "    \n",
    "    driver = webdriver.Chrome(options=chrome_options)\n",
    "    driver.implicitly_wait(10)\n",
    "    return driver\n",
    "\n",
    "def extract_stock_number(stock_text):\n",
    "    \"\"\"Extrait le nombre de livres disponibles du texte de stock\"\"\"\n",
    "    if not stock_text:\n",
    "        return 0\n",
    "    \n",
    "    # Recherche d'un nombre dans le texte\n",
    "    match = re.search(r'(\\d+)', stock_text)\n",
    "    if match:\n",
    "        return int(match.group(1))\n",
    "    \n",
    "    # Si \"In stock\" sans nombre, consid√©rer comme 1\n",
    "    if \"in stock\" in stock_text.lower():\n",
    "        return 1\n",
    "    \n",
    "    return 0\n",
    "\n",
    "def clean_description(description):\n",
    "    \"\"\"Nettoie la description du livre\"\"\"\n",
    "    if not description:\n",
    "        return \"\"\n",
    "    \n",
    "    # Supprimer les espaces multiples\n",
    "    description = re.sub(r'\\s+', ' ', description)\n",
    "    \n",
    "    # Supprimer les caract√®res sp√©ciaux ind√©sirables\n",
    "    description = re.sub(r'[^\\w\\s.,!?;:\\'-]', '', description)\n",
    "    \n",
    "    # Supprimer \"...more\" √† la fin\n",
    "    description = re.sub(r'\\.\\.\\.more$', '', description)\n",
    "    \n",
    "    return description.strip()\n",
    "\n",
    "def scrape_book_details(driver, book_url):\n",
    "    \"\"\"Scrape les d√©tails d'un livre sp√©cifique\"\"\"\n",
    "    try:\n",
    "        driver.get(book_url)\n",
    "        wait = WebDriverWait(driver, 10)\n",
    "        \n",
    "        # Attendre que la page soit charg√©e\n",
    "        wait.until(EC.presence_of_element_located((By.TAG_NAME, \"h1\")))\n",
    "        \n",
    "        # Titre\n",
    "        titre = driver.find_element(By.TAG_NAME, \"h1\").text\n",
    "        \n",
    "        # Description\n",
    "        description = \"\"\n",
    "        try:\n",
    "            desc_element = driver.find_element(By.CSS_SELECTOR, \"#product_description + p\")\n",
    "            description = desc_element.text\n",
    "        except NoSuchElementException:\n",
    "            description = titre  # Utiliser le titre si pas de description\n",
    "        \n",
    "        # Prix\n",
    "        prix_text = driver.find_element(By.CSS_SELECTOR, \".price_color\").text\n",
    "        prix = float(re.sub(r'[^\\d.]', '', prix_text))\n",
    "        \n",
    "        # Disponibilit√©\n",
    "        stock_text = driver.find_element(By.CSS_SELECTOR, \".instock\").text\n",
    "        disponibilite = extract_stock_number(stock_text)\n",
    "        \n",
    "        # Image URL\n",
    "        img_element = driver.find_element(By.CSS_SELECTOR, \"#product_gallery img\")\n",
    "        image_url = urljoin(BASE_URL, img_element.get_attribute(\"src\"))\n",
    "        \n",
    "        # Note\n",
    "        note = 0\n",
    "        try:\n",
    "            rating_element = driver.find_element(By.CSS_SELECTOR, \".star-rating\")\n",
    "            rating_class = rating_element.get_attribute(\"class\")\n",
    "            for word in rating_class.split():\n",
    "                if word in RATING_MAP:\n",
    "                    note = RATING_MAP[word]\n",
    "                    break\n",
    "        except NoSuchElementException:\n",
    "            pass\n",
    "        \n",
    "        return {\n",
    "            'titre': titre,\n",
    "            'description': clean_description(description),\n",
    "            'prix': prix,\n",
    "            'disponibilite': disponibilite,\n",
    "            'image_url': image_url,\n",
    "            'note': note\n",
    "        }\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Erreur lors du scraping de {book_url}: {e}\")\n",
    "        return None\n",
    "\n",
    "def scrape_all_books():\n",
    "    \"\"\"Scrape tous les livres du site\"\"\"\n",
    "    driver = setup_driver()\n",
    "    livres = []\n",
    "    \n",
    "    try:\n",
    "        print(\"D√©but du scraping...\")\n",
    "        \n",
    "        page = 1\n",
    "        while True:\n",
    "            url = f\"{BASE_URL}/catalogue/page-{page}.html\"\n",
    "            print(f\"Scraping page {page}...\")\n",
    "            \n",
    "            driver.get(url)\n",
    "            \n",
    "            # V√©rifier si la page existe\n",
    "            try:\n",
    "                WebDriverWait(driver, 5).until(\n",
    "                    EC.presence_of_element_located((By.CSS_SELECTOR, \".product_pod\"))\n",
    "                )\n",
    "            except TimeoutException:\n",
    "                print(f\"Page {page} non trouv√©e. Arr√™t du scraping.\")\n",
    "                break\n",
    "            \n",
    "            # R√©cup√©rer tous les liens des livres sur cette page\n",
    "            book_links = []\n",
    "            book_elements = driver.find_elements(By.CSS_SELECTOR, \".product_pod h3 a\")\n",
    "            \n",
    "            for element in book_elements:\n",
    "                book_href = element.get_attribute(\"href\")\n",
    "                if book_href:\n",
    "                    book_links.append(book_href)\n",
    "            \n",
    "            print(f\"Trouv√© {len(book_links)} livres sur la page {page}\")\n",
    "            \n",
    "            # Scraper chaque livre\n",
    "            for i, book_url in enumerate(book_links, 1):\n",
    "                print(f\"  Scraping livre {i}/{len(book_links)}...\")\n",
    "                book_data = scrape_book_details(driver, book_url)\n",
    "                \n",
    "                if book_data:\n",
    "                    livres.append(book_data)\n",
    "                \n",
    "                # Petite pause pour √©viter de surcharger le serveur\n",
    "                time.sleep(0.5)\n",
    "            \n",
    "            page += 1\n",
    "            \n",
    "            # Limiter √† 50 pages pour √©viter une boucle infinie\n",
    "            if page > 50:\n",
    "                break\n",
    "    \n",
    "    finally:\n",
    "        driver.quit()\n",
    "    \n",
    "    print(f\"Scraping termin√©. {len(livres)} livres r√©cup√©r√©s.\")\n",
    "    return livres\n",
    "\n",
    "def save_to_csv(livres, filename):\n",
    "    \"\"\"Sauvegarde les donn√©es dans un fichier CSV\"\"\"\n",
    "    df = pd.DataFrame(livres)\n",
    "    df.to_csv(filename, index=False, encoding='utf-8')\n",
    "    print(f\"‚úÖ Donn√©es sauvegard√©es dans {filename}\")\n",
    "    return df\n",
    "\n",
    "def create_database_table(engine):\n",
    "    \"\"\"Cr√©e la table dans la base de donn√©es\"\"\"\n",
    "    Base.metadata.create_all(engine)\n",
    "    print(\"‚úÖ Table 'livres' cr√©√©e dans la base de donn√©es\")\n",
    "\n",
    "def save_to_database(df, engine):\n",
    "    \"\"\"Sauvegarde les donn√©es dans la base de donn√©es\"\"\"\n",
    "    try:\n",
    "        df.to_sql('livres', engine, if_exists='replace', index=False)\n",
    "        print(\"‚úÖ Donn√©es sauvegard√©es dans PostgreSQL\")\n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Erreur lors de la sauvegarde dans PostgreSQL: {e}\")\n",
    "\n",
    "def preprocess_data(df):\n",
    "    \"\"\"Pr√©traite les donn√©es\"\"\"\n",
    "    print(\"Pr√©traitement des donn√©es...\")\n",
    "    \n",
    "    # 1. Nettoyer la description (d√©j√† fait dans clean_description)\n",
    "    df['description'] = df['description'].fillna('')\n",
    "    \n",
    "    # 2. Prix d√©j√† converti en float\n",
    "    \n",
    "    # 3. Disponibilit√© d√©j√† convertie en int\n",
    "    \n",
    "    # 4. Note d√©j√† extraite sous forme num√©rique\n",
    "    \n",
    "    # 5. Remplir les valeurs manquantes de description avec le titre\n",
    "    mask = (df['description'] == '') | df['description'].isna()\n",
    "    df.loc[mask, 'description'] = df.loc[mask, 'titre']\n",
    "    \n",
    "    print(\"‚úÖ Pr√©traitement termin√©\")\n",
    "    return df\n",
    "\n",
    "def create_recommendation_model(df):\n",
    "    \"\"\"Cr√©e le mod√®le de recommandation bas√© sur la similarit√© cosinus\"\"\"\n",
    "    print(\"Cr√©ation du mod√®le de recommandation...\")\n",
    "    \n",
    "    # Pr√©parer les descriptions pour TF-IDF\n",
    "    descriptions = df['description'].fillna('').astype(str)\n",
    "    \n",
    "    # Appliquer TF-IDF\n",
    "    vectorizer = TfidfVectorizer(\n",
    "        max_features=5000,\n",
    "        stop_words='english',\n",
    "        lowercase=True,\n",
    "        ngram_range=(1, 2)\n",
    "    )\n",
    "    \n",
    "    tfidf_matrix = vectorizer.fit_transform(descriptions)\n",
    "    \n",
    "    # Calculer la matrice de similarit√© cosinus\n",
    "    similarity_matrix = cosine_similarity(tfidf_matrix)\n",
    "    \n",
    "    # Sauvegarder le mod√®le\n",
    "    model_data = {\n",
    "        'vectorizer': vectorizer,\n",
    "        'tfidf_matrix': tfidf_matrix,\n",
    "        'similarity_matrix': similarity_matrix,\n",
    "        'titles': df['titre'].tolist(),\n",
    "        'indices': pd.Series(df.index, index=df['titre']).to_dict()\n",
    "    }\n",
    "    \n",
    "    joblib.dump(model_data, 'recommendation_model.pkl')\n",
    "    print(\"‚úÖ Mod√®le de recommandation sauvegard√© dans 'recommendation_model.pkl'\")\n",
    "    \n",
    "    return model_data\n",
    "\n",
    "def get_recommendations(titre, model_data, n_recommendations=5):\n",
    "    \"\"\"Obtient des recommandations pour un livre donn√©\"\"\"\n",
    "    try:\n",
    "        # Obtenir l'index du livre\n",
    "        idx = model_data['indices'][titre]\n",
    "        \n",
    "        # Obtenir les scores de similarit√©\n",
    "        sim_scores = list(enumerate(model_data['similarity_matrix'][idx]))\n",
    "        \n",
    "        # Trier par score de similarit√© (d√©croissant)\n",
    "        sim_scores = sorted(sim_scores, key=lambda x: x[1], reverse=True)\n",
    "        \n",
    "        # Obtenir les indices des livres similaires (exclure le livre lui-m√™me)\n",
    "        sim_indices = [i[0] for i in sim_scores[1:n_recommendations+1]]\n",
    "        \n",
    "        # Retourner les titres des livres recommand√©s\n",
    "        return [model_data['titles'][i] for i in sim_indices]\n",
    "        \n",
    "    except KeyError:\n",
    "        return f\"Livre '{titre}' non trouv√© dans la base de donn√©es\"\n",
    "\n",
    "def main():\n",
    "    \"\"\"Fonction principale\"\"\"\n",
    "    print(\"=== SCRAPING BOOKS TO SCRAPE AVEC SELENIUM ===\\n\")\n",
    "    \n",
    "    # 1. Scraper les donn√©es\n",
    "    livres = scrape_all_books()\n",
    "    \n",
    "    if not livres:\n",
    "        print(\"‚ùå Aucune donn√©e r√©cup√©r√©e. Arr√™t du programme.\")\n",
    "        return\n",
    "    \n",
    "    # 2. Cr√©er le DataFrame\n",
    "    df = pd.DataFrame(livres)\n",
    "    \n",
    "    # 3. Pr√©traiter les donn√©es\n",
    "    df = preprocess_data(df)\n",
    "    \n",
    "    # 4. Sauvegarder dans le CSV\n",
    "    save_to_csv(df, CSV_FILE)\n",
    "    \n",
    "    # 5. Sauvegarder dans la base de donn√©es\n",
    "    try:\n",
    "        engine = create_engine(DB_URI)\n",
    "        create_database_table(engine)\n",
    "        save_to_database(df, engine)\n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Erreur avec la base de donn√©es: {e}\")\n",
    "    \n",
    "    # 6. Cr√©er le mod√®le de recommandation\n",
    "    model_data = create_recommendation_model(df)\n",
    "    \n",
    "    # 7. Exemple de recommandations\n",
    "    if len(df) > 0:\n",
    "        premier_livre = df.iloc[0]['titre']\n",
    "        recommendations = get_recommendations(premier_livre, model_data)\n",
    "        print(f\"\\nüìö Recommandations pour '{premier_livre}':\")\n",
    "        for i, rec in enumerate(recommendations, 1):\n",
    "            print(f\"  {i}. {rec}\")\n",
    "    \n",
    "    print(\"\\n‚úÖ Processus termin√© avec succ√®s!\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "320b0dd7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "D√©but du scraping...\n",
      "Scraping page 1/50...\n",
      "Scraping page 2/50...\n",
      "Scraping page 3/50...\n",
      "Scraping page 4/50...\n",
      "Scraping page 5/50...\n",
      "Scraping page 6/50...\n",
      "Scraping page 7/50...\n",
      "Scraping page 8/50...\n",
      "Scraping page 9/50...\n",
      "Scraping page 10/50...\n",
      "Scraping page 11/50...\n",
      "Scraping page 12/50...\n",
      "Scraping page 13/50...\n",
      "Scraping page 14/50...\n",
      "Scraping page 15/50...\n",
      "Scraping page 16/50...\n",
      "Scraping page 17/50...\n",
      "Scraping page 18/50...\n",
      "Scraping page 19/50...\n",
      "Scraping page 20/50...\n",
      "Scraping page 21/50...\n",
      "Scraping page 22/50...\n",
      "Scraping page 23/50...\n",
      "Scraping page 24/50...\n",
      "Scraping page 25/50...\n",
      "Scraping page 26/50...\n",
      "Scraping page 27/50...\n",
      "Scraping page 28/50...\n",
      "Scraping page 29/50...\n",
      "Scraping page 30/50...\n",
      "Scraping page 31/50...\n",
      "Scraping page 32/50...\n",
      "Scraping page 33/50...\n",
      "Scraping page 34/50...\n",
      "Scraping page 35/50...\n",
      "Scraping page 36/50...\n",
      "Scraping page 37/50...\n",
      "Scraping page 38/50...\n",
      "Scraping page 39/50...\n",
      "Scraping page 40/50...\n",
      "Scraping page 41/50...\n",
      "Scraping page 42/50...\n",
      "Scraping page 43/50...\n",
      "Scraping page 44/50...\n",
      "Scraping page 45/50...\n",
      "Scraping page 46/50...\n",
      "Scraping page 47/50...\n",
      "Scraping page 48/50...\n",
      "Scraping page 49/50...\n",
      "Scraping page 50/50...\n",
      "Scraping termin√©. 1000 livres r√©cup√©r√©s.\n",
      "\n",
      "Aper√ßu des donn√©es avec ID :\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>titre</th>\n",
       "      <th>description</th>\n",
       "      <th>prix</th>\n",
       "      <th>disponibilite</th>\n",
       "      <th>image_url</th>\n",
       "      <th>note</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>A Light in the Attic</td>\n",
       "      <td>It's hard to imagine a world without A Light i...</td>\n",
       "      <td>51.77</td>\n",
       "      <td>0</td>\n",
       "      <td>https://books.toscrape.com/media/cache/2c/da/2...</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>Tipping the Velvet</td>\n",
       "      <td>\"Erotic and absorbing...Written with starling ...</td>\n",
       "      <td>53.74</td>\n",
       "      <td>0</td>\n",
       "      <td>https://books.toscrape.com/media/cache/26/0c/2...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>Soumission</td>\n",
       "      <td>Dans une France assez proche de la n√¥tre, un h...</td>\n",
       "      <td>50.10</td>\n",
       "      <td>0</td>\n",
       "      <td>https://books.toscrape.com/media/cache/3e/ef/3...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>Sharp Objects</td>\n",
       "      <td>WICKED above her hipbone, GIRL across her hear...</td>\n",
       "      <td>47.82</td>\n",
       "      <td>0</td>\n",
       "      <td>https://books.toscrape.com/media/cache/32/51/3...</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>Sapiens: A Brief History of Humankind</td>\n",
       "      <td>From a renowned historian comes a groundbreaki...</td>\n",
       "      <td>54.23</td>\n",
       "      <td>0</td>\n",
       "      <td>https://books.toscrape.com/media/cache/be/a5/b...</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   id                                  titre  \\\n",
       "0   0                   A Light in the Attic   \n",
       "1   1                     Tipping the Velvet   \n",
       "2   2                             Soumission   \n",
       "3   3                          Sharp Objects   \n",
       "4   4  Sapiens: A Brief History of Humankind   \n",
       "\n",
       "                                         description   prix  disponibilite  \\\n",
       "0  It's hard to imagine a world without A Light i...  51.77              0   \n",
       "1  \"Erotic and absorbing...Written with starling ...  53.74              0   \n",
       "2  Dans une France assez proche de la n√¥tre, un h...  50.10              0   \n",
       "3  WICKED above her hipbone, GIRL across her hear...  47.82              0   \n",
       "4  From a renowned historian comes a groundbreaki...  54.23              0   \n",
       "\n",
       "                                           image_url  note  \n",
       "0  https://books.toscrape.com/media/cache/2c/da/2...     3  \n",
       "1  https://books.toscrape.com/media/cache/26/0c/2...     1  \n",
       "2  https://books.toscrape.com/media/cache/3e/ef/3...     1  \n",
       "3  https://books.toscrape.com/media/cache/32/51/3...     4  \n",
       "4  https://books.toscrape.com/media/cache/be/a5/b...     5  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Aper√ßu des donn√©es:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>titre</th>\n",
       "      <th>description</th>\n",
       "      <th>prix</th>\n",
       "      <th>disponibilite</th>\n",
       "      <th>image_url</th>\n",
       "      <th>note</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>A Light in the Attic</td>\n",
       "      <td>It's hard to imagine a world without A Light i...</td>\n",
       "      <td>51.77</td>\n",
       "      <td>0</td>\n",
       "      <td>https://books.toscrape.com/media/cache/2c/da/2...</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>Tipping the Velvet</td>\n",
       "      <td>\"Erotic and absorbing...Written with starling ...</td>\n",
       "      <td>53.74</td>\n",
       "      <td>0</td>\n",
       "      <td>https://books.toscrape.com/media/cache/26/0c/2...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>Soumission</td>\n",
       "      <td>Dans une France assez proche de la n√¥tre, un h...</td>\n",
       "      <td>50.10</td>\n",
       "      <td>0</td>\n",
       "      <td>https://books.toscrape.com/media/cache/3e/ef/3...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>Sharp Objects</td>\n",
       "      <td>WICKED above her hipbone, GIRL across her hear...</td>\n",
       "      <td>47.82</td>\n",
       "      <td>0</td>\n",
       "      <td>https://books.toscrape.com/media/cache/32/51/3...</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>Sapiens: A Brief History of Humankind</td>\n",
       "      <td>From a renowned historian comes a groundbreaki...</td>\n",
       "      <td>54.23</td>\n",
       "      <td>0</td>\n",
       "      <td>https://books.toscrape.com/media/cache/be/a5/b...</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   id                                  titre  \\\n",
       "0   0                   A Light in the Attic   \n",
       "1   1                     Tipping the Velvet   \n",
       "2   2                             Soumission   \n",
       "3   3                          Sharp Objects   \n",
       "4   4  Sapiens: A Brief History of Humankind   \n",
       "\n",
       "                                         description   prix  disponibilite  \\\n",
       "0  It's hard to imagine a world without A Light i...  51.77              0   \n",
       "1  \"Erotic and absorbing...Written with starling ...  53.74              0   \n",
       "2  Dans une France assez proche de la n√¥tre, un h...  50.10              0   \n",
       "3  WICKED above her hipbone, GIRL across her hear...  47.82              0   \n",
       "4  From a renowned historian comes a groundbreaki...  54.23              0   \n",
       "\n",
       "                                           image_url  note  \n",
       "0  https://books.toscrape.com/media/cache/2c/da/2...     3  \n",
       "1  https://books.toscrape.com/media/cache/26/0c/2...     1  \n",
       "2  https://books.toscrape.com/media/cache/3e/ef/3...     1  \n",
       "3  https://books.toscrape.com/media/cache/32/51/3...     4  \n",
       "4  https://books.toscrape.com/media/cache/be/a5/b...     5  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Nombre de livres: 1000\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\"Colonnes: ['id', 'titre', 'description', 'prix', 'disponibilite', 'image_url', 'note']\""
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "'Types: id                 int64\\ntitre             object\\ndescription       object\\nprix             float64\\ndisponibilite      int64\\nimage_url         object\\nnote               int64\\ndtype: object'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚ùå Erreur lors de la sauvegarde dans PostgreSQL : (psycopg2.errors.DependentObjectsStillExist) ERREUR:  n'a pas pu supprimer table livres car d'autres objets en d√©pendent\n",
      "DETAIL:  contrainte emprunts_id_livre_fkey sur table emprunts d√©pend de table livres\n",
      "contrainte reservations_id_livre_fkey sur table reservations d√©pend de table livres\n",
      "HINT:  Utilisez DROP ... CASCADE pour supprimer aussi les objets d√©pendants.\n",
      "\n",
      "[SQL: \n",
      "DROP TABLE livres]\n",
      "(Background on this error at: https://sqlalche.me/e/20/2j85)\n",
      "‚úÖ Donn√©es √©galement sauvegard√©es dans livres.csv\n"
     ]
    }
   ],
   "source": [
    "import csv\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "from sqlalchemy import create_engine\n",
    "from urllib.parse import urljoin\n",
    "\n",
    "# -----------------------\n",
    "# Config PostgreSQL\n",
    "# -----------------------\n",
    "DB_URI = \"postgresql+psycopg2://postgres:admin@localhost:5432/booksdb\"\n",
    "\n",
    "base_url = \"https://books.toscrape.com/catalogue/page-{}.html\"\n",
    "site_base = \"https://books.toscrape.com/\"\n",
    "livres = []\n",
    "\n",
    "rating_map = {\n",
    "    \"One\": 1,\n",
    "    \"Two\": 2,\n",
    "    \"Three\": 3,\n",
    "    \"Four\": 4,\n",
    "    \"Five\": 5\n",
    "}\n",
    "\n",
    "# ======= SCRAPING =======\n",
    "print(\"D√©but du scraping...\")\n",
    "for page in range(1, 51):\n",
    "    url = base_url.format(page)\n",
    "    print(f\"Scraping page {page}/50...\")\n",
    "    response = requests.get(url)\n",
    "    response.encoding = \"latin-1\"\n",
    "    \n",
    "    soup = BeautifulSoup(response.text, \"html.parser\")\n",
    "    articles = soup.find_all(\"article\", class_=\"product_pod\")\n",
    "    \n",
    "    for article in articles:\n",
    "        try:\n",
    "            # Titre\n",
    "            titre = article.h3.a[\"title\"]\n",
    "            titre = titre.encode(\"latin-1\", errors=\"ignore\").decode(\"utf-8\", errors=\"ignore\")\n",
    "            \n",
    "            # Prix\n",
    "            prix_str = article.find(\"p\", class_=\"price_color\").text.strip()\n",
    "            prix_str = prix_str.replace(\"√Ç\", \"\").replace(\"¬£\", \"\")\n",
    "            prix = float(prix_str)\n",
    "            \n",
    "            # Stock/Disponibilit√©\n",
    "            stock_text = article.find(\"p\", class_=\"instock availability\").text.strip()\n",
    "            # Extract number from \"In stock (22 available)\"\n",
    "            import re\n",
    "            stock_match = re.search(r'\\((\\d+)', stock_text)\n",
    "            disponibilite = int(stock_match.group(1)) if stock_match else 0\n",
    "            \n",
    "            # Image URL\n",
    "            img_url = article.find(\"img\")[\"src\"]\n",
    "            img_url = urljoin(site_base, img_url)\n",
    "            \n",
    "            # Rating\n",
    "            rating_class = article.find(\"p\", class_=\"star-rating\")[\"class\"][1]\n",
    "            note = rating_map.get(rating_class, 0)\n",
    "            \n",
    "            # --- R√©cup√©ration de la description ---\n",
    "            detail_href = article.h3.a[\"href\"]\n",
    "            detail_url = urljoin(site_base + \"catalogue/\", detail_href)\n",
    "            detail_resp = requests.get(detail_url)\n",
    "            detail_resp.encoding = \"latin-1\"\n",
    "            detail_soup = BeautifulSoup(detail_resp.text, \"html.parser\")\n",
    "            \n",
    "            desc_tag = detail_soup.find(\"div\", id=\"product_description\")\n",
    "            if desc_tag:\n",
    "                description = desc_tag.find_next_sibling(\"p\").text.strip()\n",
    "                description = description.encode(\"latin-1\", errors=\"ignore\").decode(\"utf-8\", errors=\"ignore\")\n",
    "            else:\n",
    "                description = \"\"\n",
    "            \n",
    "            livres.append({\n",
    "                'titre': titre,\n",
    "                'description': description,\n",
    "                'prix': prix,\n",
    "                'disponibilite': disponibilite,\n",
    "                'image_url': img_url,\n",
    "                'note': note\n",
    "            })\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Erreur lors du scraping d'un article: {e}\")\n",
    "            continue\n",
    "\n",
    "print(f\"Scraping termin√©. {len(livres)} livres r√©cup√©r√©s.\")\n",
    "\n",
    "# ======= CHARGEMENT DIRECT EN DB =======\n",
    "try:\n",
    "    # Cr√©er DataFrame directement\n",
    "    df = pd.DataFrame(livres)\n",
    "    \n",
    "\n",
    "    # Ajouter une colonne 'id' √† partir de l'index\n",
    "    df.reset_index(inplace=True)\n",
    "    df.rename(columns={'index': 'id'}, inplace=True)\n",
    "\n",
    "    # V√©rifier les donn√©es\n",
    "    print(\"\\nAper√ßu des donn√©es avec ID :\")\n",
    "    display(df.head())\n",
    "\n",
    "    \n",
    "    # V√©rifier les donn√©es\n",
    "    print(\"\\nAper√ßu des donn√©es:\")\n",
    "    display(df.head())\n",
    "    print(f\"\\nNombre de livres: {len(df)}\")\n",
    "    display(f\"Colonnes: {list(df.columns)}\")\n",
    "    display(f\"Types: {df.dtypes}\")\n",
    "    \n",
    "    # Connexion PostgreSQL via SQLAlchemy\n",
    "    engine = create_engine(DB_URI)\n",
    "    \n",
    "    # Sauvegarde dans la table \"livres\" \n",
    "    df.to_sql(\"livres\", engine, if_exists=\"replace\", index=False)\n",
    "    \n",
    "    print(\"‚úÖ Donn√©es sauvegard√©es dans PostgreSQL avec to_sql()\")\n",
    "    \n",
    "    # V√©rification\n",
    "    verification_query = \"SELECT COUNT(*) as total FROM livres\"\n",
    "    result = pd.read_sql_query(verification_query, engine)\n",
    "    print(f\"‚úÖ V√©rification: {result['total'].iloc[0]} livres dans la base\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(\"‚ùå Erreur lors de la sauvegarde dans PostgreSQL :\", e)\n",
    "\n",
    "# ======= SAUVEGARDE CSV OPTIONNELLE =======\n",
    "try:\n",
    "    csv_file = \"livres.csv\"\n",
    "    df.to_csv(csv_file, index=False, encoding=\"utf-8\")\n",
    "    print(f\"‚úÖ Donn√©es √©galement sauvegard√©es dans {csv_file}\")\n",
    "except Exception as e:\n",
    "    print(f\"‚ùå Erreur sauvegarde CSV: {e}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
